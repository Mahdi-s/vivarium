{
  "paths_config": "paths.json",
  "suite_name": "olmo_conformity_expanded_resume_temp0.4",
  "suite_version": "v5_resume",
  "description": "RESUME run for T=0.4. Completes models that did not finish in the original v2 run (original run_id: 0d0eb2bd-7575-4945-85fa-a9ddc38f8c3a). Only includes incomplete variants: instruct_dpo, think, think_sft, think_dpo, rl_zero.",
  "datasets": [
    {
      "name": "immutable_facts_minimal",
      "version": "v2",
      "path": "experiments/olmo_conformity/datasets/immutable_facts/minimal_items_wrong.jsonl",
      "category": "general",
      "notes": "Curated factual questions (geography, science, math)"
    },
    {
      "name": "social_conventions_minimal",
      "version": "v2",
      "path": "experiments/olmo_conformity/datasets/social_conventions/minimal_items_wrong.jsonl",
      "category": "opinion",
      "notes": "Opinion/preference questions"
    },
    {
      "name": "gsm8k",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/math/gsm8k_items_wrong.jsonl",
      "category": "math",
      "notes": "Grade school math word problems - tests RL-Zero's training domain"
    },
    {
      "name": "mmlu_math",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/math/mmlu_math_items_wrong.jsonl",
      "category": "math",
      "notes": "MMLU math subsets (high school, elementary, college)"
    },
    {
      "name": "mmlu_science",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/science/mmlu_science_items_wrong.jsonl",
      "category": "science",
      "notes": "MMLU science subsets (physics, chemistry, biology)"
    },
    {
      "name": "mmlu_knowledge",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/knowledge/mmlu_knowledge_items_wrong.jsonl",
      "category": "knowledge",
      "notes": "MMLU knowledge subsets (geography, history, world religions)"
    },
    {
      "name": "truthfulqa",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/truthfulness/truthfulqa_items_wrong.jsonl",
      "category": "truthfulness",
      "notes": "TruthfulQA - tests tendency to reproduce common misconceptions"
    },
    {
      "name": "arc",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/reasoning/arc_items_wrong.jsonl",
      "category": "reasoning",
      "notes": "ARC Challenge + Easy - science reasoning questions"
    }
  ],
  "conditions": [
    {
      "name": "control",
      "params": {
        "type": "control"
      },
      "notes": "Baseline: direct question without social pressure"
    },
    {
      "name": "asch_history_5",
      "params": {
        "type": "synthetic_asch_history",
        "confederates": 5,
        "confidence": "high"
      },
      "notes": "Asch paradigm: 5 confederates claim wrong_answer with high confidence"
    },
    {
      "name": "asch_zhu_unbiased_unanimous_plain",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "unanimous",
        "tone": "plain"
      },
      "notes": "Zhu et al.-style Asch: unbiased system + unanimous-plain confederates"
    },
    {
      "name": "asch_zhu_unbiased_unanimous_neutral",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "unanimous",
        "tone": "neutral"
      },
      "notes": "Zhu et al.-style Asch: unbiased system + unanimous-neutral (more natural) confederates"
    },
    {
      "name": "asch_zhu_unbiased_unanimous_confident",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "unanimous",
        "tone": "confident"
      },
      "notes": "Zhu et al.-style Asch: unbiased system + unanimous-confident confederates"
    },
    {
      "name": "asch_zhu_unbiased_unanimous_uncertain",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "unanimous",
        "tone": "uncertain"
      },
      "notes": "Zhu et al.-style Asch: unbiased system + unanimous-uncertain confederates"
    },
    {
      "name": "asch_zhu_unbiased_diverse_plain",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "diverse",
        "tone": "plain"
      },
      "notes": "Zhu et al.-style control: Diverse answers (no majority) with same dialogue format"
    },
    {
      "name": "asch_zhu_unbiased_qd",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "unanimous",
        "tone": "plain",
        "distillation": true
      },
      "notes": "Zhu et al.-style intervention: Question Distillation (summarize unanimous answer, reduce repetition)"
    },
    {
      "name": "asch_zhu_unbiased_da",
      "params": {
        "type": "asch_peer_pressure",
        "system_style": "control",
        "prompt_style": "conversation",
        "confederates": 5,
        "consensus": "unanimous",
        "tone": "plain",
        "devils_advocate": true
      },
      "notes": "Zhu et al.-style intervention: Devil's Advocate (one dissenter breaks unanimity)"
    },
    {
      "name": "authoritative_bias",
      "params": {
        "type": "authoritative_bias",
        "user_claim_strength": "high"
      },
      "notes": "Authoritative user claims wrong_answer with high confidence"
    },
    {
      "name": "authority_zhu_unbiased_trust",
      "params": {
        "type": "authority_claim",
        "system_style": "control",
        "claim_style": "trust"
      },
      "notes": "Zhu et al. appendix sycophancy-style authority claim: 'I trust that the answer must be ...' (unbiased system)"
    },
    {
      "name": "authority_zhu_unbiased_trust_da",
      "params": {
        "type": "authority_claim",
        "system_style": "control",
        "claim_style": "trust",
        "devils_advocate": true
      },
      "notes": "Zhu et al. appendix mitigation: add an alternate (incorrect) option to reduce sycophancy (DA)"
    }
  ],
  "models": [
    {
      "variant": "instruct_dpo",
      "model_id": "allenai/Olmo-3-7B-Instruct-DPO"
    },
    {
      "variant": "think",
      "model_id": "allenai/Olmo-3-7B-Think"
    },
    {
      "variant": "think_sft",
      "model_id": "allenai/Olmo-3-7B-Think-SFT"
    },
    {
      "variant": "think_dpo",
      "model_id": "allenai/Olmo-3-7B-Think-DPO"
    },
    {
      "variant": "rl_zero",
      "model_id": "allenai/Olmo-3-7B-RL-Zero-Math"
    }
  ],
  "run": {
    "seed": 42,
    "temperature": 0.4,
    "top_k": 50,
    "top_p": 0.9,
    "max_items_per_dataset": 200,
    "notes": "temperature=0.4 for moderate-low sampling variance; top_k=50/top_p=0.9 locked; seed=42 for reproducibility"
  },
  "scientific_notes": {
    "validation_gates": [
      "Prompt gate: Every Asch/authoritative trial must use wrong_answer != ground_truth_text",
      "Temperature 0.4: Moderate-low variance sampling",
      "Category analysis: Results should be analyzed by dataset category to test domain effects"
    ],
    "categories": [
      "general",
      "opinion",
      "math",
      "science",
      "knowledge",
      "truthfulness",
      "reasoning"
    ]
  },
  "_original_run_id": "0d0eb2bd-7575-4945-85fa-a9ddc38f8c3a"
}