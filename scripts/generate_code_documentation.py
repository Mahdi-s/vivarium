#!/usr/bin/env python3
"""
Generate comprehensive code documentation for Vivarium.

This script uses repo-to-text to extract all code from src/ and appends
comprehensive documentation about data structures, persistence mechanisms,
retrieval methods, and analysis tools.
"""

from __future__ import annotations

import json
import os
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Check if repo-to-text CLI is available
REPO_TO_TEXT_AVAILABLE = False
try:
    import subprocess
    result = subprocess.run(
        ["repo-to-text", "--help"],
        capture_output=True,
        text=True,
        timeout=5
    )
    REPO_TO_TEXT_AVAILABLE = (result.returncode == 0 or "usage" in result.stdout.lower() or "usage" in result.stderr.lower())
except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
    REPO_TO_TEXT_AVAILABLE = False

if not REPO_TO_TEXT_AVAILABLE:
    print("WARNING: repo-to-text CLI not found. Will use manual file extraction.")
    print("To install: pip install repo-to-text")


def extract_code_manually(project_root: Path) -> str:
    """Fallback: manually extract code from src/ directory."""
    src_dir = project_root / "src"
    if not src_dir.exists():
        return "# Error: src/ directory not found\n"
    
    code_parts = []
    code_parts.append("=" * 80 + "\n")
    code_parts.append("SOURCE CODE FROM src/aam/ (Manual Extraction)\n")
    code_parts.append("=" * 80 + "\n\n")
    
    # Walk through src/ directory
    for root, dirs, files in os.walk(str(src_dir)):
        # Skip __pycache__ and other hidden directories
        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']
        
        for file in sorted(files):
            if file.endswith('.py'):
                file_path = Path(root) / file
                rel_path = file_path.relative_to(project_root)
                code_parts.append(f"\n{'=' * 80}\n")
                code_parts.append(f"File: {rel_path}\n")
                code_parts.append(f"{'=' * 80}\n\n")
                try:
                    content = file_path.read_text(encoding="utf-8")
                    code_parts.append(content)
                    code_parts.append("\n")
                except Exception as e:
                    code_parts.append(f"# Error reading file: {e}\n")
    
    return "".join(code_parts)


def get_documentation_sections() -> str:
    """Generate hardcoded documentation sections about the codebase structure."""
    
    doc = """
================================================================================
VIVARIUM - COMPREHENSIVE CODE DOCUMENTATION
================================================================================

This document contains all source code from src/aam/ along with detailed
documentation about data structures, persistence mechanisms, retrieval methods,
and post-simulation analysis tools.

Generated by: scripts/generate_code_documentation.py

================================================================================
SECTION 1: RUN DIRECTORY STRUCTURE
================================================================================

Vivarium organizes experiment runs in a structured directory
hierarchy under the 'runs/' directory:

runs/<timestamp>_<run_id>/
├── simulation.db                    # SQLite database (all metadata and trace events)
├── run_metadata.json                # Full experiment configuration, git hash, dependencies
├── experiment_config.json           # Copy of the input experiment config (if applicable)
├── activations/                     # Activation shard files (safetensors format)
│   ├── step_000000.safetensors
│   ├── step_000001.safetensors
│   ├── step_000001__<suffix>.safetensors  # Collision-handled files (unique suffix)
│   └── ... (one file per time_step, or more if collisions occur)
├── artifacts/                       # Analysis artifacts
│   ├── truth_probe.safetensors     # Trained truth probe weights
│   ├── social_probe.safetensors    # Trained social probe weights
│   ├── figures/                     # Generated analysis figures (PNG)
│   │   ├── conformity_rate_by_variant.png
│   │   ├── vector_collision_by_layer.png
│   │   └── ...
│   └── tables/                       # Generated analysis tables (CSV)
│       ├── conformity_rate_by_variant.csv
│       └── ...
├── exports/                          # Parquet exports (if enabled)
│   ├── trace.parquet                # Trace events in columnar format
│   └── messages.parquet            # Messages in columnar format
└── logs/                            # System and LLM audit logs (if enabled)
    ├── system.log
    └── llm_audit.log

Directory Naming Convention:
- Format: <YYYYMMDD>_<HHMMSS>_<run_id>
- Example: 20251217_002021_b2cc39a5-3d9d-444d-8489-bb74d6946973
- run_id is a UUID v4 generated at experiment start

================================================================================
SECTION 2: DATABASE SCHEMA (SQLite)
================================================================================

The simulation.db SQLite database contains the following tables:

CORE TABLES:
------------

1. runs
   - run_id (TEXT PRIMARY KEY): Unique identifier for the experiment run
   - seed (INTEGER): Random seed for reproducibility
   - created_at (REAL): Timestamp (seconds since epoch)
   - config_json (TEXT): Full experiment configuration as JSON

2. trace
   - trace_id (TEXT PRIMARY KEY): UUID for each trace event
   - run_id (TEXT): Foreign key to runs
   - time_step (INTEGER): Logical clock time of the simulation
   - agent_id (TEXT): The internal simulation identity of the agent
   - action_type (TEXT): Stable identifier for the tool/action
   - info_json (TEXT): Action payload as JSON
   - outcome_json (TEXT): Action outcome payload as JSON
   - created_at (REAL): Wall-clock timestamp
   - environment_state_hash (TEXT): Optional integrity hash of environment state
   - Indexes: idx_trace_run_step, idx_trace_agent

3. messages
   - message_id (TEXT PRIMARY KEY): UUID for each message
   - run_id (TEXT): Foreign key to runs
   - time_step (INTEGER): Time step when message was posted
   - author_id (TEXT): Agent who posted the message
   - content (TEXT): Message text content
   - created_at (REAL): Timestamp
   - Indexes: idx_messages_run_step, idx_messages_run_author

4. activation_metadata
   - record_id (TEXT PRIMARY KEY): UUID for each activation record
   - run_id (TEXT): Foreign key to runs
   - time_step (INTEGER): Time step when activation was captured
   - agent_id (TEXT): Agent identifier
   - model_id (TEXT): Model version used
   - layer_index (INTEGER): Transformer layer index
   - component (TEXT): Hook component name (e.g., "hook_resid_post")
   - token_position (INTEGER): Token position (-1 for last token)
   - shard_file_path (TEXT): Path to safetensors file
   - tensor_key (TEXT): Key within safetensors file
   - shape_json (TEXT): Tensor shape as JSON array
   - dtype (TEXT): Tensor dtype (e.g., "float16", "float32")
   - created_at (REAL): Timestamp
   - Indexes: idx_activation_run_step, idx_activation_agent, idx_activation_layer

5. merkle_log
   - merkle_id (TEXT PRIMARY KEY): UUID for each Merkle log entry
   - run_id (TEXT): Foreign key to runs
   - time_step (INTEGER): Time step
   - agent_id (TEXT): Agent identifier
   - prompt_hash (TEXT): SHA256 hash of prompt
   - activation_hash (TEXT): SHA256 hash of activation tensor
   - leaf_hash (TEXT): Combined leaf hash
   - merkle_root (TEXT): Merkle tree root at this step
   - created_at (REAL): Timestamp
   - Index: idx_merkle_run_step_agent

OLMO CONFORMITY EXPERIMENT TABLES:
-----------------------------------

6. conformity_datasets
   - dataset_id (TEXT PRIMARY KEY): UUID for dataset
   - name (TEXT): Dataset name (e.g., "immutable_facts_minimal")
   - version (TEXT): Dataset version (e.g., "v1")
   - path (TEXT): File system path to dataset JSONL file
   - sha256 (TEXT): SHA256 hash of dataset file
   - created_at (REAL): Timestamp
   - Index: idx_conformity_datasets_name

7. conformity_items
   - item_id (TEXT PRIMARY KEY): UUID for item
   - dataset_id (TEXT): Foreign key to conformity_datasets
   - domain (TEXT): Item domain/category
   - question (TEXT): Question text
   - ground_truth_text (TEXT): Ground truth answer (optional)
   - ground_truth_json (TEXT): Ground truth as JSON (optional)
   - source_json (TEXT): Source metadata as JSON
   - created_at (REAL): Timestamp
   - Index: idx_conformity_items_dataset

8. conformity_conditions
   - condition_id (TEXT PRIMARY KEY): UUID for condition
   - name (TEXT): Condition name (e.g., "control", "asch_history_5")
   - params_json (TEXT): Condition parameters as JSON
   - created_at (REAL): Timestamp
   - Index: idx_conformity_conditions_name

9. conformity_trials
   - trial_id (TEXT PRIMARY KEY): UUID for trial
   - run_id (TEXT): Foreign key to runs
   - model_id (TEXT): Model identifier (e.g., "allenai/Olmo-3-1025-7B")
   - variant (TEXT): Model variant (e.g., "base", "instruct")
   - item_id (TEXT): Foreign key to conformity_items
   - condition_id (TEXT): Foreign key to conformity_conditions
   - seed (INTEGER): Random seed for this trial
   - temperature (REAL): Sampling temperature
   - created_at (REAL): Timestamp
   - Indexes: idx_conformity_trials_run, idx_conformity_trials_item

10. conformity_prompts
    - prompt_id (TEXT PRIMARY KEY): UUID for prompt
    - trial_id (TEXT): Foreign key to conformity_trials
    - system_prompt (TEXT): System prompt text
    - user_prompt (TEXT): User prompt text
    - chat_history_json (TEXT): Chat history as JSON array
    - rendered_prompt_hash (TEXT): SHA256 hash of rendered prompt
    - created_at (REAL): Timestamp
    - Index: idx_conformity_prompts_trial

11. conformity_trial_steps
    - trial_id (TEXT PRIMARY KEY): Foreign key to conformity_trials
    - time_step (INTEGER): Time step for activation capture alignment
    - agent_id (TEXT): Agent identifier
    - created_at (REAL): Timestamp
    - Index: idx_conformity_trial_steps_step

12. conformity_outputs
    - output_id (TEXT PRIMARY KEY): UUID for output
    - trial_id (TEXT): Foreign key to conformity_trials
    - raw_text (TEXT): Raw model response text
    - parsed_answer_text (TEXT): Parsed answer (first line, normalized)
    - parsed_answer_json (TEXT): Parsed answer as JSON (may contain judge eval scores)
    - is_correct (INTEGER): Boolean correctness (0/1, NULL if not applicable)
    - refusal_flag (INTEGER): Boolean refusal flag (0/1)
    - latency_ms (REAL): Response latency in milliseconds
    - token_usage_json (TEXT): Token usage statistics as JSON
    - created_at (REAL): Timestamp
    - Indexes: idx_conformity_outputs_trial, idx_conformity_outputs_correct

13. conformity_probes
    - probe_id (TEXT PRIMARY KEY): UUID for probe
    - run_id (TEXT): Foreign key to runs
    - probe_kind (TEXT): Probe type (e.g., "truth", "social")
    - train_dataset_id (TEXT): Foreign key to conformity_datasets
    - model_id (TEXT): Model identifier
    - layers_json (TEXT): List of layer indices as JSON array
    - component (TEXT): Hook component name (e.g., "hook_resid_post")
    - token_position (INTEGER): Token position
    - artifact_path (TEXT): Path to probe weights safetensors file
    - metrics_json (TEXT): Training metrics as JSON
    - created_at (REAL): Timestamp
    - Index: idx_conformity_probes_run

14. conformity_probe_projections
    - projection_id (TEXT PRIMARY KEY): UUID for projection
    - trial_id (TEXT): Foreign key to conformity_trials
    - probe_id (TEXT): Foreign key to conformity_probes
    - layer_index (INTEGER): Layer index
    - token_index (INTEGER): Token index (optional)
    - value_float (REAL): Scalar projection value
    - created_at (REAL): Timestamp
    - Indexes: idx_conformity_proj_trial, idx_conformity_proj_layer

15. conformity_think_tokens
    - think_id (TEXT PRIMARY KEY): UUID for think token
    - trial_id (TEXT): Foreign key to conformity_trials
    - token_index (INTEGER): Token position in sequence
    - token_text (TEXT): Token text
    - token_id (INTEGER): Token ID in vocabulary
    - created_at (REAL): Timestamp
    - Index: idx_conformity_think_trial

16. conformity_logit_lens
    - logit_id (TEXT PRIMARY KEY): UUID for logit lens entry
    - trial_id (TEXT): Foreign key to conformity_trials
    - layer_index (INTEGER): Layer index
    - token_index (INTEGER): Token position
    - topk_json (TEXT): Top-k token predictions as JSON array
    - created_at (REAL): Timestamp
    - Index: idx_conformity_logit_trial

17. conformity_interventions
    - intervention_id (TEXT PRIMARY KEY): UUID for intervention
    - run_id (TEXT): Foreign key to runs
    - name (TEXT): Intervention name (e.g., "sycophancy_switch_alpha_1.0")
    - alpha (REAL): Steering coefficient
    - target_layers_json (TEXT): Target layer indices as JSON array
    - component (TEXT): Hook component name
    - vector_probe_id (TEXT): Foreign key to conformity_probes (social probe)
    - notes (TEXT): Optional notes
    - created_at (REAL): Timestamp
    - Index: idx_conformity_interventions_run

18. conformity_intervention_results
    - result_id (TEXT PRIMARY KEY): UUID for result
    - trial_id (TEXT): Foreign key to conformity_trials
    - intervention_id (TEXT): Foreign key to conformity_interventions
    - output_id_before (TEXT): Foreign key to conformity_outputs (before intervention)
    - output_id_after (TEXT): Foreign key to conformity_outputs (after intervention)
    - flipped_to_truth (INTEGER): Boolean flag if intervention flipped answer to truth
    - created_at (REAL): Timestamp
    - Index: idx_conformity_intervention_trial

================================================================================
SECTION 3: DATA FORMATS AND SHAPES
================================================================================

SAFETENSORS FILES (Activation Storage):
---------------------------------------

File Naming Convention:
- Base format: step_<time_step:06d>.safetensors
- Collision format: step_<time_step:06d>__<suffix>.safetensors
  (suffix is 8-character hex UUID when time_step collision occurs)

Example files:
- step_000000.safetensors
- step_000001.safetensors
- step_000001__555d8e6e.safetensors  (collision-handled)

Tensor Keys:
- Format: <agent_id>.<hook_name>
- Examples:
  * trial_b5896eaa.blocks.10.hook_resid_post
  * probe_agent.blocks.15.hook_resid_post

Tensor Shapes:
- Residual stream: [d_model] (e.g., [4096] for Olmo-3-7B)
- Attention outputs: [num_heads, head_dim] or [d_model]
- MLP outputs: [d_model]

Tensor Dtype:
- Default: torch.float16
- Alternative: torch.float32 (configurable via CaptureConfig)

Embedded Metadata (in safetensors header):
{
  "run_id": "<uuid>",
  "step_id": "<time_step>",
  "model_id": "<model_identifier>",
  "provenance_hash": "<sha256_of_tensor_data>",
  "merkle_root_at_step": "<merkle_tree_root>"
}

JSON CONFIG FILES:
------------------

1. Experiment Config (experiment.json):
{
  "run": {
    "steps": 50,
    "agents": 10,
    "seed": 42,
    "deterministic_timestamps": true,
    "runs_dir": "./runs"
  },
  "scheduler": {
    "per_agent_timeout_s": 30.0,
    "max_concurrency": 10,
    "sort_mode": "agent_id"
  },
  "policy": {
    "kind": "cognitive",
    "model": "gpt-3.5-turbo",
    "mock_llm": false,
    "message_history": 20
  }
}

2. Olmo Conformity Suite Config (suite_small.json):
{
  "suite_name": "olmo_conformity_small",
  "datasets": [
    {
      "name": "immutable_facts_minimal",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/immutable_facts/minimal_items.jsonl"
    },
    {
      "name": "social_conventions_minimal",
      "version": "v1",
      "path": "experiments/olmo_conformity/datasets/social_conventions/minimal_items.jsonl"
    }
  ],
  "conditions": [
    { "name": "control", "params": { "type": "control" } },
    { "name": "asch_history_5", "params": { "type": "synthetic_asch_history", "confederates": 5, "confidence": "high" } },
    { "name": "authoritative_bias", "params": { "type": "authoritative_bias", "user_claim_strength": "high" } }
  ],
  "models": [
    { "variant": "base", "model_id": "allenai/Olmo-3-1025-7B" }
  ],
  "run": {
    "seed": 42,
    "temperature": 0.0,
    "max_items_per_dataset": 10
  }
}

3. Run Metadata (run_metadata.json):
{
  "run_id": "<uuid>",
  "created_at": <timestamp>,
  "argv": ["<command_line_args>"],
  "git_hash": "<git_commit_hash>",
  "python": {
    "version": "<python_version_string>"
  },
  "config": { <full_experiment_config> },
  "dependencies": [
    { "name": "<package_name>", "version": "<version>" },
    ...
  ]
}

PARQUET EXPORTS:
----------------

Trace Parquet Schema:
- trace_id: string
- run_id: string
- time_step: int64
- timestamp: float64
- agent_id: string
- action_type: string
- info_json: string (JSON serialized)
- outcome_json: string (JSON serialized)
- environment_state_hash: string (nullable)

Messages Parquet Schema:
- message_id: string
- run_id: string
- time_step: int64
- author_id: string
- content: string
- created_at: float64

DATASET JSONL FORMAT:
---------------------

Each line is a JSON object:
{
  "domain": "<category>",
  "question": "<question_text>",
  "ground_truth": "<answer>",
  "source": { <metadata> }
}

================================================================================
SECTION 4: PERSISTENCE MECHANISMS
================================================================================

1. TraceDb Class (src/aam/persistence.py):
   - SQLite database wrapper for trace events and metadata
   - Methods:
     * init_schema(): Creates all tables and indexes
     * insert_run(meta): Inserts run metadata
     * append_trace(event): Appends trace event
     * insert_message(...): Inserts message
     * fetch_recent_messages(...): Retrieves message history
     * fetch_trace_events(...): Retrieves trace events for replay
     * insert_activation_metadata(record): Indexes activation tensor
     * insert_merkle_log(...): Logs Merkle tree entry
     * fetch_activation_metadata(...): Queries activation metadata

2. CaptureContext Class (src/aam/interpretability.py):
   - Buffers activations during model inference
   - Commits activations based on action decision
   - Flushes to safetensors files per time step
   - Handles time_step collisions with unique suffixes
   - Embeds metadata in safetensors headers
   - Indexes activations in activation_metadata table

3. Export Functions (src/aam/export.py):
   - export_trace_to_parquet(): Exports trace events to Parquet
   - export_messages_to_parquet(): Exports messages to Parquet
   - export_table_to_parquet(): Generic Parquet export for any table

4. Probe Persistence (src/aam/experiments/olmo_conformity/probes.py):
   - train_probe_from_captured_activations(): Trains probe, saves weights as safetensors
   - compute_and_store_probe_projections_for_trials(): Computes and stores projections
   - Probe weights stored in artifacts/ directory
   - Projections stored in conformity_probe_projections table

================================================================================
SECTION 5: RETRIEVAL MECHANISMS
================================================================================

1. Database Queries (via TraceDb):
   - fetch_trace_events(run_id, from_time_step, to_time_step): Get trace events
   - fetch_recent_messages(run_id, up_to_time_step, limit): Get message history
   - fetch_activation_metadata(run_id, time_step, agent_id): Query activation index
   - get_run_metadata(run_id): Get run configuration

2. Activation Loading:
   - Query activation_metadata table for shard_file_path and tensor_key
   - Load safetensors file using safetensors library
   - Access tensor by key: tensors[tensor_key]
   - Shape and dtype available in metadata record

3. Message History:
   - fetch_recent_messages() returns list of dicts ordered oldest-to-newest
   - Each dict contains: message_id, run_id, time_step, author_id, content, created_at
   - Used for building agent observations in cognitive policies

4. Trace Replay:
   - fetch_trace_events() returns List[TraceEvent] ordered by time_step
   - Each TraceEvent contains: trace_id, run_id, time_step, agent_id, action_type, info, outcome
   - Can be used to reconstruct simulation state

5. Probe Projections:
   - Query conformity_probe_projections table
   - Filter by trial_id, probe_id, layer_index
   - Returns scalar projection values (value_float)

================================================================================
SECTION 6: POST-SIMULATION ANALYSIS
================================================================================

ANALYTICS MODULES (src/aam/analytics/):
----------------------------------------

1. behavioral.py:
   - compute_behavioral_metrics(): Computes correctness rates, refusal rates by condition
   - generate_behavioral_graphs(): Creates bar charts, line plots
   - export_behavioral_logs(): Exports metrics to JSON/CSV

2. probes.py:
   - compute_probe_metrics(): Analyzes probe projection statistics
   - generate_probe_graphs(): Creates projection heatmaps, layer-wise plots
   - export_probe_logs(): Exports probe metrics

3. interventions.py:
   - compute_intervention_metrics(): Measures intervention effectiveness
   - generate_intervention_graphs(): Creates before/after comparison plots
   - export_intervention_logs(): Exports intervention results

4. judgeval.py:
   - compute_judgeval_metrics(): Analyzes judge eval scores (conformity, truthfulness, rationalization)
   - generate_judgeval_graphs(): Creates score distribution plots
   - export_judgeval_logs(): Exports judge eval metrics

5. think_tokens.py:
   - compute_think_metrics(): Analyzes think token patterns
   - generate_think_graphs(): Creates think token visualizations
   - export_think_logs(): Exports think token data

6. activations.py:
   - compute_activation_stats(): Computes activation statistics (mean, std, norms)
   - generate_activation_graphs(): Creates activation distribution plots
   - export_activation_logs(): Exports activation statistics

PROBE TRAINING AND PROJECTION:
-------------------------------

Location: src/aam/experiments/olmo_conformity/probes.py

Process:
1. capture_probe_dataset_to_db(): Runs model over labeled dataset, captures activations
2. train_probe_from_captured_activations(): Trains logistic regression probe
3. compute_and_store_probe_projections_for_trials(): Projects trial activations onto probe vector
4. Probe weights saved as safetensors in artifacts/ directory
5. Projections stored in conformity_probe_projections table

VECTOR ANALYSIS:
----------------

Location: src/aam/experiments/olmo_conformity/vector_analysis.py

Functions:
- run_truth_social_vector_analysis(): Complete workflow for truth vs social vector analysis
- Analyzes collision between truth and social vectors layer-by-layer
- Detects "Turn" layers where social vector suppresses truth vector
- Generates vector collision plots

INTERVENTION ANALYSIS:
----------------------

Location: src/aam/experiments/olmo_conformity/intervention.py

Process:
1. Load social probe vector
2. For each trial, load activation at target layer
3. Steer activation: activation_steered = activation + alpha * probe_vector
4. Re-run model with steered activation
5. Compare before/after outputs
6. Store results in conformity_intervention_results table

LOGIT LENS ANALYSIS:
--------------------

Location: src/aam/experiments/olmo_conformity/logit_lens.py

Process:
1. For each trial, extract logits at each layer
2. Compute top-k token predictions at each layer
3. Store in conformity_logit_lens table as JSON
4. Enables analysis of how token predictions evolve through layers

JUDGE EVAL SCORING:
--------------------

Location: src/aam/experiments/olmo_conformity/judgeval_scorers.py

Scorers:
- ConformityScorer: Detects sycophancy and conformity patterns (0-1 scale)
- TruthfulnessScorer: Evaluates factual accuracy (0-1 scale)
- RationalizationScorer: Analyzes reasoning quality for Think models (0-1 scale)

Process:
1. Load model outputs from conformity_outputs table
2. Send to local Ollama judge model via API
3. Parse scores from judge response
4. Store in parsed_answer_json field as JSON: {"conformity": 0.69, "truthfulness": 0.34, "rationalization": 0.0}

ORCHESTRATION:
--------------

Location: src/aam/experiments/olmo_conformity/orchestration.py

Function: run_full_experiment(config)
- Coordinates entire experiment pipeline
- Runs behavioral trials
- Captures activations
- Trains probes
- Computes projections
- Runs interventions (optional)
- Generates analysis reports

================================================================================
SECTION 7: SAMPLE JSON CONFIGS
================================================================================

See Section 3 for full JSON config examples.

Additional sample data structures:

Judge Eval Score JSON (stored in conformity_outputs.parsed_answer_json):
{
  "conformity": 0.69,
  "truthfulness": 0.34,
  "rationalization": 0.0
}

Probe Metrics JSON (stored in conformity_probes.metrics_json):
{
  "train_accuracy": 0.85,
  "train_loss": 0.12,
  "n_train_samples": 140,
  "n_features": 4096
}

Top-K Tokens JSON (stored in conformity_logit_lens.topk_json):
[
  {"token": "Paris", "prob": 0.45, "logit": 2.3},
  {"token": "London", "prob": 0.23, "logit": 1.2},
  ...
]

================================================================================
END OF DOCUMENTATION SECTIONS
================================================================================

"""
    return doc


def main():
    """Generate the comprehensive code documentation file."""
    
    # Get project root
    project_root = Path(__file__).parent.parent
    output_file = project_root / "abstract_agent_machine_code_documentation.txt"
    
    print(f"Generating code documentation...")
    print(f"Project root: {project_root}")
    print(f"Output file: {output_file}")
    
    # Use repo-to-text to extract code from src/
    # The .repo-to-text-settings.yaml file will ensure only src/ is included
    print("\nExtracting code from src/ using repo-to-text...")
    code_text = ""
    
    if REPO_TO_TEXT_AVAILABLE:
        try:
            # Use subprocess to call repo-to-text CLI
            import subprocess
            import tempfile
            import glob
            import time
            
            # Use --stdout flag to get output directly
            result = subprocess.run(
                ["repo-to-text", str(project_root), "--stdout"],
                capture_output=True,
                text=True,
                check=True,
                cwd=str(project_root),
                timeout=120
            )
            
            # Check if we got output from stdout
            if result.stdout and len(result.stdout.strip()) > 100:
                code_text = result.stdout
                print(f"Extracted {len(code_text)} characters of code via repo-to-text CLI (stdout)")
            else:
                # If stdout is empty, repo-to-text may have written to a file
                # Look for the generated file (format: repo-to-text_*.txt)
                output_files = sorted(glob.glob(str(project_root / "repo-to-text_*.txt")), reverse=True)
                if output_files:
                    # Get the most recent file
                    latest_file = output_files[0]
                    code_text = Path(latest_file).read_text(encoding="utf-8")
                    # Clean up the generated file
                    Path(latest_file).unlink()
                    print(f"Extracted {len(code_text)} characters of code via repo-to-text CLI (from file)")
                else:
                    raise FileNotFoundError("repo-to-text did not generate output")
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired) as e:
            print(f"CLI method failed: {e}")
            print(f"Attempting fallback: reading files directly from src/...")
            code_text = extract_code_manually(project_root)
            print(f"Extracted {len(code_text)} characters of code (manual extraction)")
    else:
        print("Using fallback: reading files directly from src/...")
        code_text = extract_code_manually(project_root)
        print(f"Extracted {len(code_text)} characters of code (manual extraction)")
    
    # Get documentation sections
    print("\nGenerating documentation sections...")
    doc_sections = get_documentation_sections()
    
    # Combine code and documentation
    print("\nCombining code and documentation...")
    full_documentation = f"""{doc_sections}

================================================================================
SECTION 8: SOURCE CODE FROM src/aam/
================================================================================

The following section contains all source code extracted from the src/ directory
using repo-to-text. This includes all Python modules, classes, functions, and
implementation details.

{code_text}

================================================================================
END OF SOURCE CODE
================================================================================

Documentation generated successfully.
"""
    
    # Write to file
    print(f"\nWriting to {output_file}...")
    try:
        output_file.write_text(full_documentation, encoding="utf-8")
        print(f"✓ Successfully generated {output_file}")
        print(f"  File size: {len(full_documentation):,} characters")
    except Exception as e:
        print(f"ERROR: Failed to write output file: {e}")
        sys.exit(1)
    
    print("\nDone!")


if __name__ == "__main__":
    main()
