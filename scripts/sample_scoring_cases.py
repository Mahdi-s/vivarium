#!/usr/bin/env python3
"""
Stratified sampling of raw model outputs from runs-hpc-full for scoring validation.

This script is meant to support "show me the receipts" review:
- Pulls trials from the simulation DBs under runs-hpc-full/runs/
- Stratifies by (variant, condition, dataset_category)
- Samples a small number per group and writes a Markdown report

It does NOT change any labels; it only surfaces evidence for manual inspection.
"""

from __future__ import annotations

import argparse
import json
import sqlite3
from pathlib import Path
from typing import Dict, List

import pandas as pd


DATASET_TO_CATEGORY = {
    "immutable_facts_minimal": "general",
    "social_conventions_minimal": "opinion",
    "gsm8k": "math",
    "mmlu_math": "math",
    "mmlu_science": "science",
    "mmlu_knowledge": "knowledge",
    "truthfulqa": "truthfulness",
    "arc": "reasoning",
}


def load_runs_metadata(metadata_path: Path) -> Dict[float, Dict[str, str]]:
    meta = json.loads(metadata_path.read_text())
    out: Dict[float, Dict[str, str]] = {}
    for temp_str, info in meta.get("experiments", {}).items():
        if info.get("status") != "completed":
            continue
        out[float(temp_str)] = {"run_id": str(info["run_id"]), "run_dir": str(info["run_dir"])}
    return dict(sorted(out.items(), key=lambda kv: kv[0]))


def load_all_trials(*, runs_dir: Path, runs: Dict[float, Dict[str, str]]) -> pd.DataFrame:
    rows: List[pd.DataFrame] = []
    for temp, info in runs.items():
        db = runs_dir / info["run_dir"] / "simulation.db"
        if not db.exists():
            raise SystemExit(f"Missing DB: {db}")
        conn = sqlite3.connect(str(db))
        try:
            df = pd.read_sql_query(
                """
                SELECT
                    t.trial_id,
                    t.variant,
                    c.name AS condition_name,
                    d.name AS dataset_name,
                    i.item_id,
                    i.ground_truth_text,
                    json_extract(i.source_json, '$.wrong_answer') AS wrong_answer,
                    o.parsed_answer_text,
                    o.is_correct,
                    o.refusal_flag
                FROM conformity_trials t
                JOIN conformity_conditions c ON c.condition_id = t.condition_id
                JOIN conformity_items i ON i.item_id = t.item_id
                JOIN conformity_datasets d ON d.dataset_id = i.dataset_id
                JOIN conformity_outputs o ON o.trial_id = t.trial_id
                WHERE t.run_id = ?
                  AND c.name IN ('control', 'asch_history_5', 'authoritative_bias')
                """,
                conn,
                params=[info["run_id"]],
            )
        finally:
            conn.close()

        df["temperature"] = float(temp)
        rows.append(df)
    out = pd.concat(rows, ignore_index=True)
    out["dataset_category"] = out["dataset_name"].map(DATASET_TO_CATEGORY).fillna("unknown")
    out["is_factual"] = out["ground_truth_text"].notna()
    return out


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--runs-dir", type=str, default="runs-hpc-full/runs")
    ap.add_argument("--metadata", type=str, default="Comparing_Experiments/runs_metadata.json")
    ap.add_argument("--out-md", type=str, default="tmp/scoring_samples.md")
    ap.add_argument("--n-per-group", type=int, default=2, help="Samples per (variant, condition, category)")
    ap.add_argument("--seed", type=int, default=0)
    ap.add_argument("--include-opinion", action="store_true", help="Also sample opinion items (no ground truth).")
    args = ap.parse_args()

    runs = load_runs_metadata(Path(args.metadata))
    df = load_all_trials(runs_dir=Path(args.runs_dir), runs=runs)

    if not args.include_opinion:
        df = df[df["is_factual"]].copy()

    # Stable sampling
    df = df.sample(frac=1.0, random_state=args.seed).reset_index(drop=True)

    groups = ["variant", "condition_name", "dataset_category"]
    samples = df.groupby(groups, as_index=False).head(args.n_per_group).copy()
    samples = samples.sort_values(groups + ["temperature", "item_id"])

    out_lines: List[str] = []
    out_lines.append("# Scoring Samples (runs-hpc-full)")
    out_lines.append("")
    out_lines.append("This file is generated by `scripts/sample_scoring_cases.py`.")
    out_lines.append("")

    for (_, r) in samples.iterrows():
        out_lines.append("## Case")
        out_lines.append(f"- Temperature: `{r['temperature']}`")
        out_lines.append(f"- Variant: `{r['variant']}`")
        out_lines.append(f"- Condition: `{r['condition_name']}`")
        out_lines.append(f"- Dataset: `{r['dataset_name']}` (category `{r['dataset_category']}`)")
        out_lines.append(f"- Item ID: `{r['item_id']}`")
        out_lines.append(f"- DB is_correct: `{r['is_correct']}`")
        out_lines.append(f"- refusal_flag: `{r['refusal_flag']}`")
        if pd.notna(r.get("ground_truth_text")):
            out_lines.append(f"- Ground truth: `{str(r['ground_truth_text'])}`")
        if pd.notna(r.get("wrong_answer")):
            out_lines.append(f"- wrong_answer: `{str(r['wrong_answer'])}`")
        out_lines.append("")
        ans = str(r["parsed_answer_text"] or "")
        head = ans[:600]
        tail = ans[-600:] if len(ans) > 600 else ""
        out_lines.append("**parsed_answer_text (head)**")
        out_lines.append("")
        out_lines.append("```text")
        out_lines.append(head)
        out_lines.append("```")
        if tail:
            out_lines.append("")
            out_lines.append("**parsed_answer_text (tail)**")
            out_lines.append("")
            out_lines.append("```text")
            out_lines.append(tail)
            out_lines.append("```")
        out_lines.append("")

    out_path = Path(args.out_md)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(out_lines))
    print(f"Wrote: {out_path}")
    print(f"Rows sampled: {len(samples)} (n_per_group={args.n_per_group})")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

