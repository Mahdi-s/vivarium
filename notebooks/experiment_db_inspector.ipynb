{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment DB Inspector (Prompts & Outputs)\n",
        "\n",
        "This notebook is for **manual auditing** of the exact prompts and model outputs stored in the run databases used by the paper.\n",
        "\n",
        "Itâ€™s designed to help you:\n",
        "- Discover all run DBs in `runs-hpc-full/runs` (temperature sweep)\n",
        "- Inspect full `system_prompt`, `user_prompt`, and model `raw_text` verbatim\n",
        "- Randomly sample trials (with filters) to sanity-check responses\n",
        "- View a single item across **conditions** (Control / Asch / Authority)\n",
        "- Compare a single item across **temperatures** (by switching run DBs)\n",
        "- Browse per-run artifacts (CSVs, JSON metrics, figures)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quickstart\n",
        "\n",
        "1. Run the cells top-to-bottom.\n",
        "2. If your runs live somewhere else, change `RUNS_BASE_DIR`.\n",
        "3. Use `sample_trials(...)`, `show_trial(...)`, `show_item_across_conditions(...)`, and `compare_across_temperatures(...)`.\n",
        "\n",
        "Notes:\n",
        "- Some variants emit `<think>...</think>` blocks in `raw_text`. Use `strip_think=True` (default) to hide those while auditing.\n",
        "- This notebook only reads from the databases and artifacts on disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import html\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sqlite3\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Any, Iterable, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import HTML, Image, Markdown, display\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # optional\n",
        "except Exception:\n",
        "    widgets = None\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"pyproject.toml\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "RUNS_BASE_DIR = (REPO_ROOT / \"runs-hpc-full\" / \"runs\").resolve()\n",
        "PAPER_TEX_PATH = (REPO_ROOT / \"paper\" / \"paper.tex\").resolve()\n",
        "\n",
        "print(\"REPO_ROOT:\", REPO_ROOT)\n",
        "print(\"RUNS_BASE_DIR:\", RUNS_BASE_DIR)\n",
        "print(\"PAPER_TEX_PATH:\", PAPER_TEX_PATH)\n",
        "print(\"ipywidgets:\", \"available\" if widgets is not None else \"not installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class RunInfo:\n",
        "    run_dir: Path\n",
        "    db_path: Path\n",
        "    run_id: Optional[str]\n",
        "    temperature: Optional[float]\n",
        "    n_trials: int\n",
        "    n_outputs: int\n",
        "    variants: list[str]\n",
        "    conditions: list[str]\n",
        "    datasets: list[str]\n",
        "\n",
        "\n",
        "def connect_sqlite(db_path: Path) -> sqlite3.Connection:\n",
        "    conn = sqlite3.connect(str(db_path))\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "\n",
        "def _scalar(conn: sqlite3.Connection, query: str, params: tuple[Any, ...] = ()) -> Any:\n",
        "    row = conn.execute(query, params).fetchone()\n",
        "    if row is None:\n",
        "        return None\n",
        "    return row[0]\n",
        "\n",
        "\n",
        "def discover_run_dirs(base_dir: Path) -> list[Path]:\n",
        "    if not base_dir.exists():\n",
        "        raise FileNotFoundError(f\"Base dir not found: {base_dir}\")\n",
        "    dirs: list[Path] = []\n",
        "    for p in sorted(base_dir.iterdir()):\n",
        "        if p.is_dir() and (p / \"simulation.db\").exists():\n",
        "            dirs.append(p)\n",
        "    return dirs\n",
        "\n",
        "\n",
        "def load_run_info(run_dir: Path) -> RunInfo:\n",
        "    db_path = run_dir / \"simulation.db\"\n",
        "    conn = connect_sqlite(db_path)\n",
        "    try:\n",
        "        run_id = _scalar(conn, \"SELECT run_id FROM runs LIMIT 1\")\n",
        "        temp = _scalar(conn, \"SELECT MIN(temperature) FROM conformity_trials\")\n",
        "        n_trials = int(_scalar(conn, \"SELECT COUNT(*) FROM conformity_trials\") or 0)\n",
        "        n_outputs = int(_scalar(conn, \"SELECT COUNT(*) FROM conformity_outputs\") or 0)\n",
        "        variants_raw = _scalar(conn, \"SELECT GROUP_CONCAT(DISTINCT variant) FROM conformity_trials\")\n",
        "        variants = sorted([v for v in (variants_raw or \"\").split(\",\") if v])\n",
        "        conds_raw = _scalar(\n",
        "            conn,\n",
        "            \"\"\"\n",
        "            SELECT GROUP_CONCAT(DISTINCT c.name)\n",
        "            FROM conformity_trials t\n",
        "            JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
        "            \"\"\",\n",
        "        )\n",
        "        conditions = sorted([c for c in (conds_raw or \"\").split(\",\") if c])\n",
        "        datasets_raw = _scalar(conn, \"SELECT GROUP_CONCAT(DISTINCT name) FROM conformity_datasets\")\n",
        "        datasets = sorted([d for d in (datasets_raw or \"\").split(\",\") if d])\n",
        "        return RunInfo(\n",
        "            run_dir=run_dir,\n",
        "            db_path=db_path,\n",
        "            run_id=run_id,\n",
        "            temperature=float(temp) if temp is not None else None,\n",
        "            n_trials=n_trials,\n",
        "            n_outputs=n_outputs,\n",
        "            variants=variants,\n",
        "            conditions=conditions,\n",
        "            datasets=datasets,\n",
        "        )\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "\n",
        "run_dirs = discover_run_dirs(RUNS_BASE_DIR)\n",
        "run_infos = [load_run_info(d) for d in run_dirs]\n",
        "\n",
        "runs_index = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"temperature\": ri.temperature,\n",
        "            \"run_dir\": str(ri.run_dir),\n",
        "            \"run_id\": ri.run_id,\n",
        "            \"n_trials\": ri.n_trials,\n",
        "            \"n_outputs\": ri.n_outputs,\n",
        "            \"variants\": \",\".join(ri.variants),\n",
        "            \"conditions\": \",\".join(ri.conditions),\n",
        "            \"datasets\": \",\".join(ri.datasets),\n",
        "        }\n",
        "        for ri in run_infos\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(runs_index.sort_values([\"temperature\", \"run_dir\"], na_position=\"last\").reset_index(drop=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select A Run (One Temperature)\n",
        "\n",
        "The temperature sweep is stored as *multiple run directories* (one DB per temperature). Pick a temperature and load its DB below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pick_run_by_temperature(target_temp: float) -> RunInfo:\n",
        "    candidates = [ri for ri in run_infos if ri.temperature is not None and abs(ri.temperature - target_temp) < 1e-9]\n",
        "    if not candidates:\n",
        "        raise ValueError(f\"No run found for temperature={target_temp}. Available: {sorted(set(r.temperature for r in run_infos))}\")\n",
        "    if len(candidates) > 1:\n",
        "        print(f\"Warning: multiple runs found for temperature={target_temp}; using the first.\")\n",
        "    return candidates[0]\n",
        "\n",
        "\n",
        "SELECTED_TEMPERATURE = 0.0  # change me\n",
        "selected_run = pick_run_by_temperature(SELECTED_TEMPERATURE)\n",
        "\n",
        "conn = connect_sqlite(selected_run.db_path)\n",
        "RUN_ID = _scalar(conn, \"SELECT run_id FROM runs LIMIT 1\")\n",
        "\n",
        "print(\"Selected run_dir:\", selected_run.run_dir)\n",
        "print(\"DB:\", selected_run.db_path)\n",
        "print(\"RUN_ID:\", RUN_ID)\n",
        "print(\"Temperature:\", selected_run.temperature)\n",
        "print(\"Trials:\", selected_run.n_trials)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_run_config(conn: sqlite3.Connection) -> None:\n",
        "    config_json = _scalar(conn, \"SELECT config_json FROM runs WHERE run_id = ?\", (RUN_ID,))\n",
        "    if not config_json:\n",
        "        print(\"No config_json found in runs table.\")\n",
        "        return\n",
        "    try:\n",
        "        config = json.loads(config_json)\n",
        "    except Exception:\n",
        "        print(\"Config exists but could not be parsed as JSON.\")\n",
        "        return\n",
        "    pretty = json.dumps(config, indent=2, sort_keys=True)\n",
        "    display(HTML(f\"<h3>runs.config_json</h3><pre style='white-space: pre-wrap'>{html.escape(pretty)}</pre>\"))\n",
        "\n",
        "\n",
        "show_run_config(conn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_tables(conn: sqlite3.Connection) -> list[str]:\n",
        "    rows = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\").fetchall()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "\n",
        "def table_schema(conn: sqlite3.Connection, table: str) -> pd.DataFrame:\n",
        "    return pd.read_sql_query(f\"PRAGMA table_info({table});\", conn)\n",
        "\n",
        "\n",
        "def preview_table(conn: sqlite3.Connection, table: str, limit: int = 5) -> pd.DataFrame:\n",
        "    return pd.read_sql_query(f\"SELECT * FROM {table} LIMIT {int(limit)};\", conn)\n",
        "\n",
        "\n",
        "tables = list_tables(conn)\n",
        "print(f\"Tables ({len(tables)}):\")\n",
        "for t in tables:\n",
        "    print(\" -\", t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_trial_summary(conn: sqlite3.Connection) -> pd.DataFrame:\n",
        "    q = \"\"\"\n",
        "    SELECT\n",
        "        d.name AS dataset,\n",
        "        c.name AS condition,\n",
        "        t.variant,\n",
        "        t.temperature,\n",
        "        COUNT(*) AS n_trials,\n",
        "        AVG(o.is_correct) AS accuracy,\n",
        "        AVG(o.refusal_flag) AS refusal_rate\n",
        "    FROM conformity_trials t\n",
        "    JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
        "    JOIN conformity_items i ON i.item_id = t.item_id\n",
        "    JOIN conformity_datasets d ON d.dataset_id = i.dataset_id\n",
        "    LEFT JOIN conformity_outputs o ON o.trial_id = t.trial_id\n",
        "    WHERE t.run_id = ?\n",
        "    GROUP BY 1,2,3,4\n",
        "    ORDER BY 1,2,3,4;\n",
        "    \"\"\"\n",
        "    return pd.read_sql_query(q, conn, params=(RUN_ID,))\n",
        "\n",
        "\n",
        "summary_df = show_trial_summary(conn)\n",
        "display(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _safe_json_loads(s: Optional[str]) -> dict[str, Any]:\n",
        "    if not s:\n",
        "        return {}\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "\n",
        "def strip_think(raw_text: str) -> str:\n",
        "    \"\"\"Remove a leading <think>...</think> block if present.\"\"\"\n",
        "    if raw_text is None:\n",
        "        return \"\"\n",
        "    marker = \"</think>\"\n",
        "    if marker in raw_text:\n",
        "        return raw_text.split(marker, 1)[1].lstrip()\n",
        "    return raw_text\n",
        "\n",
        "\n",
        "def trial_query(where_sql: str = \"\", limit_sql: str = \"\") -> str:\n",
        "    return f\"\"\"\n",
        "    SELECT\n",
        "        t.trial_id,\n",
        "        t.variant,\n",
        "        t.temperature,\n",
        "        c.name AS condition,\n",
        "        d.name AS dataset,\n",
        "        i.domain,\n",
        "        t.item_id,\n",
        "        i.question,\n",
        "        i.ground_truth_text,\n",
        "        i.source_json,\n",
        "        p.system_prompt,\n",
        "        p.user_prompt,\n",
        "        o.raw_text,\n",
        "        o.parsed_answer_text,\n",
        "        o.is_correct,\n",
        "        o.refusal_flag\n",
        "    FROM conformity_trials t\n",
        "    JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
        "    JOIN conformity_items i ON i.item_id = t.item_id\n",
        "    JOIN conformity_datasets d ON d.dataset_id = i.dataset_id\n",
        "    LEFT JOIN conformity_prompts p ON p.trial_id = t.trial_id\n",
        "    LEFT JOIN conformity_outputs o ON o.trial_id = t.trial_id\n",
        "    WHERE t.run_id = ?\n",
        "    {where_sql}\n",
        "    {limit_sql}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def sample_trials(\n",
        "    conn: sqlite3.Connection,\n",
        "    n: int = 5,\n",
        "    dataset: Optional[str] = None,\n",
        "    condition: Optional[str] = None,\n",
        "    variant: Optional[str] = None,\n",
        "    is_correct: Optional[int] = None,\n",
        "    refusal_flag: Optional[int] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    strip_think_output: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    where = []\n",
        "    params: list[Any] = [RUN_ID]\n",
        "\n",
        "    if dataset is not None:\n",
        "        where.append(\"AND d.name = ?\")\n",
        "        params.append(dataset)\n",
        "    if condition is not None:\n",
        "        where.append(\"AND c.name = ?\")\n",
        "        params.append(condition)\n",
        "    if variant is not None:\n",
        "        where.append(\"AND t.variant = ?\")\n",
        "        params.append(variant)\n",
        "    if is_correct is not None:\n",
        "        where.append(\"AND o.is_correct = ?\")\n",
        "        params.append(int(is_correct))\n",
        "    if refusal_flag is not None:\n",
        "        where.append(\"AND o.refusal_flag = ?\")\n",
        "        params.append(int(refusal_flag))\n",
        "\n",
        "    df = pd.read_sql_query(trial_query(where_sql=\"\\n\".join(where), limit_sql=\"\"), conn, params=tuple(params))\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    if random_seed is not None:\n",
        "        rnd = random.Random(random_seed)\n",
        "        sample_ids = rnd.sample(list(df[\"trial_id\"]), k=min(int(n), len(df)))\n",
        "        df = df[df[\"trial_id\"].isin(sample_ids)].copy()\n",
        "    else:\n",
        "        df = df.sample(n=min(int(n), len(df)), random_state=None).copy()\n",
        "\n",
        "    if strip_think_output:\n",
        "        df[\"raw_text\"] = df[\"raw_text\"].fillna(\"\").map(strip_think)\n",
        "\n",
        "    # Helpful parsed fields\n",
        "    df[\"wrong_answer\"] = df[\"source_json\"].apply(lambda s: _safe_json_loads(s).get(\"wrong_answer\"))\n",
        "    df[\"source_notes\"] = df[\"source_json\"].apply(lambda s: _safe_json_loads(s).get(\"notes\"))\n",
        "\n",
        "    # Reorder columns for inspection\n",
        "    cols = [\n",
        "        \"trial_id\",\n",
        "        \"dataset\",\n",
        "        \"domain\",\n",
        "        \"variant\",\n",
        "        \"temperature\",\n",
        "        \"condition\",\n",
        "        \"item_id\",\n",
        "        \"question\",\n",
        "        \"ground_truth_text\",\n",
        "        \"wrong_answer\",\n",
        "        \"is_correct\",\n",
        "        \"refusal_flag\",\n",
        "        \"user_prompt\",\n",
        "        \"raw_text\",\n",
        "        \"parsed_answer_text\",\n",
        "        \"source_notes\",\n",
        "        \"system_prompt\",\n",
        "    ]\n",
        "    cols = [c for c in cols if c in df.columns]\n",
        "    return df[cols].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def show_trial(conn: sqlite3.Connection, trial_id: str, strip_think_output: bool = True) -> None:\n",
        "    q = trial_query(where_sql=\"AND t.trial_id = ?\", limit_sql=\"LIMIT 1\")\n",
        "    df = pd.read_sql_query(q, conn, params=(RUN_ID, trial_id))\n",
        "    if df.empty:\n",
        "        print(f\"Trial not found: {trial_id}\")\n",
        "        return\n",
        "    row = df.iloc[0].to_dict()\n",
        "\n",
        "    source = _safe_json_loads(row.get(\"source_json\"))\n",
        "    wrong_answer = source.get(\"wrong_answer\")\n",
        "    notes = source.get(\"notes\")\n",
        "\n",
        "    raw_text = row.get(\"raw_text\") or \"\"\n",
        "    if strip_think_output:\n",
        "        raw_text = strip_think(raw_text)\n",
        "\n",
        "    parts = []\n",
        "    parts.append(f\"<h3>Trial: {html.escape(str(trial_id))}</h3>\")\n",
        "    meta = {\n",
        "        \"dataset\": row.get(\"dataset\"),\n",
        "        \"domain\": row.get(\"domain\"),\n",
        "        \"variant\": row.get(\"variant\"),\n",
        "        \"temperature\": row.get(\"temperature\"),\n",
        "        \"condition\": row.get(\"condition\"),\n",
        "        \"item_id\": row.get(\"item_id\"),\n",
        "        \"is_correct\": row.get(\"is_correct\"),\n",
        "        \"refusal_flag\": row.get(\"refusal_flag\"),\n",
        "        \"wrong_answer\": wrong_answer,\n",
        "    }\n",
        "    parts.append(f\"<pre style='white-space: pre-wrap'>{html.escape(json.dumps(meta, indent=2))}</pre>\")\n",
        "    if notes:\n",
        "        parts.append(f\"<b>Source notes</b><pre style='white-space: pre-wrap'>{html.escape(str(notes))}</pre>\")\n",
        "\n",
        "    parts.append(f\"<b>Question</b><pre style='white-space: pre-wrap'>{html.escape(str(row.get('question') or ''))}</pre>\")\n",
        "    parts.append(f\"<b>Ground truth</b><pre style='white-space: pre-wrap'>{html.escape(str(row.get('ground_truth_text') or ''))}</pre>\")\n",
        "\n",
        "    parts.append(f\"<b>System prompt</b><pre style='white-space: pre-wrap'>{html.escape(str(row.get('system_prompt') or ''))}</pre>\")\n",
        "    parts.append(f\"<b>User prompt</b><pre style='white-space: pre-wrap'>{html.escape(str(row.get('user_prompt') or ''))}</pre>\")\n",
        "    parts.append(f\"<b>Model raw_text</b><pre style='white-space: pre-wrap'>{html.escape(str(raw_text))}</pre>\")\n",
        "    if row.get(\"parsed_answer_text\"):\n",
        "        parts.append(f\"<b>parsed_answer_text</b><pre style='white-space: pre-wrap'>{html.escape(str(row.get('parsed_answer_text') or ''))}</pre>\")\n",
        "\n",
        "    display(HTML(\"\\n\".join(parts)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Sampling (Manual Audit)\n",
        "\n",
        "Start here: sample a few trials and then call `show_trial(...)` for any row that looks suspicious.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: sample across everything in the selected DB\n",
        "df_sample = sample_trials(conn, n=5)\n",
        "display(df_sample)\n",
        "\n",
        "# To view a full transcript:\n",
        "# show_trial(conn, df_sample.loc[0, 'trial_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect One Item Across Conditions (Control / Asch / Authority)\n",
        "\n",
        "Given an `item_id` and a `variant`, this will show the prompts and outputs for all conditions *within the selected temperature DB*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_item_across_conditions(\n",
        "    conn: sqlite3.Connection,\n",
        "    item_id: str,\n",
        "    variant: str,\n",
        "    strip_think_output: bool = True,\n",
        "    condition_order: Optional[list[str]] = None,\n",
        ") -> pd.DataFrame:\n",
        "    q = trial_query(where_sql=\"AND t.item_id = ? AND t.variant = ?\", limit_sql=\"\")\n",
        "    df = pd.read_sql_query(q, conn, params=(RUN_ID, item_id, variant))\n",
        "    if df.empty:\n",
        "        print(f\"No trials found for item_id={item_id}, variant={variant}\")\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"wrong_answer\"] = df[\"source_json\"].apply(lambda s: _safe_json_loads(s).get(\"wrong_answer\"))\n",
        "    if strip_think_output:\n",
        "        df[\"raw_text\"] = df[\"raw_text\"].fillna(\"\").map(strip_think)\n",
        "\n",
        "    # Display each condition as its own block\n",
        "    conds = list(df[\"condition\"].unique())\n",
        "    if condition_order is None:\n",
        "        condition_order = [\"control\", \"asch_history_5\", \"authoritative_bias\"]\n",
        "    conds_sorted = [c for c in condition_order if c in conds] + [c for c in conds if c not in condition_order]\n",
        "\n",
        "    for cond in conds_sorted:\n",
        "        r = df[df[\"condition\"] == cond].iloc[0].to_dict()\n",
        "        title = f\"{cond} | {r.get('dataset')} | {r.get('domain')} | {r.get('variant')} | T={r.get('temperature')}\"\n",
        "        display(HTML(f\"<h3>{html.escape(title)}</h3>\"))\n",
        "        display(HTML(f\"<b>System</b><pre style='white-space: pre-wrap'>{html.escape(str(r.get('system_prompt') or ''))}</pre>\"))\n",
        "        display(HTML(f\"<b>User</b><pre style='white-space: pre-wrap'>{html.escape(str(r.get('user_prompt') or ''))}</pre>\"))\n",
        "        display(HTML(f\"<b>Output</b><pre style='white-space: pre-wrap'>{html.escape(str(r.get('raw_text') or ''))}</pre>\"))\n",
        "\n",
        "    cols = [\"trial_id\", \"condition\", \"question\", \"ground_truth_text\", \"wrong_answer\", \"is_correct\", \"refusal_flag\"]\n",
        "    cols = [c for c in cols if c in df.columns]\n",
        "    return df[cols].sort_values(\"condition\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Example:\n",
        "# show_item_across_conditions(conn, item_id=\"mmlu_high_school_chemistry_0004\", variant=\"think_sft\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search Items By Question Text\n",
        "\n",
        "This is the quickest way to locate a paper appendix example: search for a distinctive substring from the question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_items(conn: sqlite3.Connection, question_substr: str, limit: int = 50) -> pd.DataFrame:\n",
        "    q = \"\"\"\n",
        "    SELECT\n",
        "        i.item_id,\n",
        "        d.name AS dataset,\n",
        "        i.domain,\n",
        "        i.question,\n",
        "        i.ground_truth_text,\n",
        "        i.source_json\n",
        "    FROM conformity_items i\n",
        "    JOIN conformity_datasets d ON d.dataset_id = i.dataset_id\n",
        "    WHERE i.question LIKE ?\n",
        "    ORDER BY i.item_id\n",
        "    LIMIT ?;\n",
        "    \"\"\"\n",
        "    return pd.read_sql_query(q, conn, params=(f\"%{question_substr}%\", int(limit)))\n",
        "\n",
        "\n",
        "# Example:\n",
        "# display(search_items(conn, \"antimony\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare One Item Across Temperatures (Across Multiple Run DBs)\n",
        "\n",
        "This loads the matching trial (same `item_id` + `variant` + `condition`) from each temperature DB and shows the outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=16)\n",
        "def _conn_for_db(db_path_str: str) -> sqlite3.Connection:\n",
        "    # Cached connections for convenience during interactive exploration.\n",
        "    return connect_sqlite(Path(db_path_str))\n",
        "\n",
        "\n",
        "def _run_id_for_db(conn: sqlite3.Connection) -> Optional[str]:\n",
        "    return _scalar(conn, \"SELECT run_id FROM runs LIMIT 1\")\n",
        "\n",
        "\n",
        "def fetch_trial_by_item_variant_condition(\n",
        "    db_path: Path,\n",
        "    item_id: str,\n",
        "    variant: str,\n",
        "    condition_name: str,\n",
        ") -> Optional[dict[str, Any]]:\n",
        "    conn2 = _conn_for_db(str(db_path))\n",
        "    run_id2 = _run_id_for_db(conn2)\n",
        "    if not run_id2:\n",
        "        return None\n",
        "    q = trial_query(where_sql=\"AND t.item_id = ? AND t.variant = ? AND c.name = ?\", limit_sql=\"LIMIT 1\")\n",
        "    df = pd.read_sql_query(q, conn2, params=(run_id2, item_id, variant, condition_name))\n",
        "    if df.empty:\n",
        "        return None\n",
        "    return df.iloc[0].to_dict()\n",
        "\n",
        "\n",
        "def compare_across_temperatures(\n",
        "    item_id: str,\n",
        "    variant: str,\n",
        "    condition_name: str,\n",
        "    strip_think_output: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    rows: list[dict[str, Any]] = []\n",
        "    for ri in sorted([r for r in run_infos if r.temperature is not None], key=lambda r: r.temperature):\n",
        "        rec = fetch_trial_by_item_variant_condition(ri.db_path, item_id=item_id, variant=variant, condition_name=condition_name)\n",
        "        if rec is None:\n",
        "            continue\n",
        "        raw = rec.get(\"raw_text\") or \"\"\n",
        "        if strip_think_output:\n",
        "            raw = strip_think(raw)\n",
        "        source = _safe_json_loads(rec.get(\"source_json\"))\n",
        "        rows.append(\n",
        "            {\n",
        "                \"temperature\": ri.temperature,\n",
        "                \"run_dir\": str(ri.run_dir),\n",
        "                \"trial_id\": rec.get(\"trial_id\"),\n",
        "                \"dataset\": rec.get(\"dataset\"),\n",
        "                \"domain\": rec.get(\"domain\"),\n",
        "                \"question\": rec.get(\"question\"),\n",
        "                \"ground_truth_text\": rec.get(\"ground_truth_text\"),\n",
        "                \"wrong_answer\": source.get(\"wrong_answer\"),\n",
        "                \"user_prompt\": rec.get(\"user_prompt\"),\n",
        "                \"raw_text\": raw,\n",
        "                \"is_correct\": rec.get(\"is_correct\"),\n",
        "                \"refusal_flag\": rec.get(\"refusal_flag\"),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(\"temperature\").reset_index(drop=True)\n",
        "    if df.empty:\n",
        "        print(\"No matching trials found across temperatures.\")\n",
        "        return df\n",
        "\n",
        "    # Show a compact table first\n",
        "    display(df[[\"temperature\", \"trial_id\", \"is_correct\", \"refusal_flag\", \"wrong_answer\"]])\n",
        "\n",
        "    # Then show full outputs\n",
        "    for r in df.to_dict(orient=\"records\"):\n",
        "        title = f\"T={r['temperature']} | trial_id={r['trial_id']} | correct={r['is_correct']} | refusal={r['refusal_flag']}\"\n",
        "        display(HTML(f\"<h3>{html.escape(title)}</h3>\"))\n",
        "        display(HTML(f\"<b>User</b><pre style='white-space: pre-wrap'>{html.escape(str(r.get('user_prompt') or ''))}</pre>\"))\n",
        "        display(HTML(f\"<b>Output</b><pre style='white-space: pre-wrap'>{html.escape(str(r.get('raw_text') or ''))}</pre>\"))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Paper-related example (Authority prompt that can look \"incomplete\" due to the underlying question):\n",
        "# compare_across_temperatures(item_id=\"mmlu_high_school_chemistry_0004\", variant=\"think_sft\", condition_name=\"authoritative_bias\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Browse Per-Run Artifacts (CSVs / JSON / Figures)\n",
        "\n",
        "Each run directory often includes `artifacts/tables`, `artifacts/logs/tables`, and `artifacts/figures`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_run_files(run_dir: Path, max_files: int = 200) -> pd.DataFrame:\n",
        "    files: list[dict[str, Any]] = []\n",
        "    for p in sorted(run_dir.rglob(\"*\")):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        files.append(\n",
        "            {\n",
        "                \"path\": str(p),\n",
        "                \"relpath\": str(p.relative_to(run_dir)),\n",
        "                \"ext\": p.suffix.lower(),\n",
        "                \"bytes\": p.stat().st_size,\n",
        "            }\n",
        "        )\n",
        "        if len(files) >= max_files:\n",
        "            break\n",
        "    return pd.DataFrame(files)\n",
        "\n",
        "\n",
        "def preview_csv(path: Path, n: int = 20) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"CSV: {path} | shape={df.shape}\")\n",
        "    return df.head(int(n))\n",
        "\n",
        "\n",
        "def preview_json(path: Path) -> Any:\n",
        "    obj = json.loads(path.read_text())\n",
        "    pretty = json.dumps(obj, indent=2, sort_keys=True)\n",
        "    display(HTML(f\"<h3>JSON: {html.escape(str(path))}</h3><pre style='white-space: pre-wrap'>{html.escape(pretty)}</pre>\"))\n",
        "    return obj\n",
        "\n",
        "\n",
        "def show_png(path: Path, width: int = 900) -> None:\n",
        "    display(Image(filename=str(path), width=width))\n",
        "\n",
        "\n",
        "files_df = list_run_files(selected_run.run_dir)\n",
        "display(files_df)\n",
        "\n",
        "# Examples:\n",
        "# display(preview_csv(selected_run.run_dir / \"artifacts\" / \"tables\" / \"conformity_rate_by_variant.csv\"))\n",
        "# preview_json(selected_run.run_dir / \"artifacts\" / \"logs\" / \"metrics_behavioral.json\")\n",
        "# show_png(selected_run.run_dir / \"artifacts\" / \"figures\" / \"accuracy_by_condition.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Small Widget UI\n",
        "\n",
        "If `ipywidgets` is installed, this provides a quick dropdown to sample and inspect trials without editing code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if widgets is None:\n",
        "    print(\"ipywidgets is not installed. You can still use the functions above.\")\n",
        "else:\n",
        "    temps = sorted([r.temperature for r in run_infos if r.temperature is not None])\n",
        "    w_temp = widgets.Dropdown(options=temps, value=SELECTED_TEMPERATURE, description=\"Temp\")\n",
        "    w_n = widgets.IntSlider(value=3, min=1, max=20, step=1, description=\"n\")\n",
        "    w_strip = widgets.Checkbox(value=True, description=\"strip_think\")\n",
        "    w_go = widgets.Button(description=\"Sample\")\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def on_click(_):\n",
        "        global conn, RUN_ID, selected_run\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            selected_run = pick_run_by_temperature(float(w_temp.value))\n",
        "            conn = connect_sqlite(selected_run.db_path)\n",
        "            RUN_ID = _scalar(conn, \"SELECT run_id FROM runs LIMIT 1\")\n",
        "            df = sample_trials(conn, n=int(w_n.value), strip_think_output=bool(w_strip.value))\n",
        "            display(df)\n",
        "\n",
        "    w_go.on_click(on_click)\n",
        "    display(widgets.HBox([w_temp, w_n, w_strip, w_go]))\n",
        "    display(out)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
