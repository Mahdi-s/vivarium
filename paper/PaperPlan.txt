Here is the refactored research plan and draft.
A) Paper Map (Revised)
* One-Line Thesis: Social conformity in LLMs is not just a behavioral output but a measurable geometric conflict between "truth" and "social" activation vectors, resolvable by determining the "Turn Layer" where social attention overrides factual retrieval.
* Contributions:
   1. Mechanistic Evidence of Conformity: First demonstration of opposing "Truth" and "Social Consensus" vectors activating simultaneously during Asch-style conflicts.
   2. The "Turn Layer" Discovery: Identification of a specific depth (e.g., layers 20-24 in 7B models) where activation dominance flips from factual representations to social compliance.
   3. Causal Steering: Demonstration that subtracting the "Social Vector" at the Turn Layer reduces conformity rates more effectively than prompt engineering.
* Experiments:
   * Exp 1 (Probe Generalization): Validating that social probes detect abstract "consensus" pressure, not just specific prefixes (e.g., "Everyone says...").
   * Exp 2 (The "Turn" Mapping): Layer-wise dual-projection analysis during Asch trials to locate the crossover point ($P_{social} > P_{truth}$).
   * Exp 3 (Causal Steering): Intervention to suppress $V_{social}$ and recover factual accuracy.
* Datasets/Models/Tools:
   * Framework: Abstract Agent Machine (AAM) with TraceDb for artifact management.
   * Models: allenai/Olmo-3-7B-Instruct (High compliance), allenai/Olmo-3-7B-Think (CoT reasoning).
   * Datasets: immutable_facts_minimal (Ground Truth), social_probe_train (Consensus markers).
B) New Experimental Designs
Option 1: The "Geometry of Compliance" (Recommended)
* Hypothesis: Conformity occurs when the projection of the residual stream onto a "Social Vector" ($V_{soc}$) exceeds its projection onto a "Truth Vector" ($V_{truth}$) at critical "Turn Layers."
* Setup: Run Olmo-3-7B-Instruct on immutable_facts. Inject 5 confederate "wrong" answers.
* Measurement: Train linear probes on held-out data. Plot $P_{truth}$ vs $P_{soc}$ across all 32 layers.
* Outcome: Identify $L_{turn}$ where lines cross.
* Feasibility: High. Supported by suite_small.json and vector_analysis.py.
Option 2: The "Thinking" Topology
* Hypothesis: Chain-of-Thought (CoT) models (Olmo-3-7B-Think) delay or prevent the "Turn" by maintaining the Truth Vector in the CoT scratchpad, even if the final output conforms.
* Setup: Compare Olmo-3-7B-Instruct vs Olmo-3-7B-Think.
* Measurement: Track $V_{truth}$ magnitude during the generation of <think> tokens vs final answer tokens.
* Feasibility: Medium. Requires parsing <think> blocks (supported by logit_lens.py).
C) Selected Design + Full Spec
Design: Option 1 (Geometry) + Option 2 (Think Comparison). This provides a complete mechanistic narrative (how conformity happens) and a solution (how reasoning/steering fixes it).
Protocol:
1. Phase 0: Probe Training
   * Generate truth_probe_train (n=1000): Pairs of Fact vs Counter-Fact.
   * Generate social_probe_train (n=1000): Pairs of Consensus-Framing ("Everyone says X") vs Neutral ("X"). Crucial: Use 10+ distinct templates.
   * Extract activations (resid_post) at the last prompt token for Layers 0-31.
   * Train Logistic Regression probes; save weights ($W_{truth}, W_{soc}$). Validate accuracy > 90% on held-out set.
2. Phase 1: The Asch Trials
   * Run suite_small.json conditions: control vs asch_history_5.
   * Models: Olmo-3-7B-Instruct and Olmo-3-7B-Think.
   * Capture activations at the last prompt token.
3. Phase 2: Vector Analysis
   * Compute projections: $Proj_L(v) = (Activations_L \cdot W_v) / \|W_v\|$.
   * Identify Turn Layer: The first layer $L$ where $Proj_L(soc) > Proj_L(truth)$ in trials that resulted in incorrect answers.
4. Phase 3: Causal Intervention
   * Steering: For layers $L_{turn} \dots L_{turn+4}$, add $-\alpha \cdot V_{soc}$ (negative social vector).
   * Control: Add random vector of same magnitude.
   * Success Metric: % of trials flipped from "Conformity" to "Truth".
Statistical Plan:
* Compare mean $L_{turn}$ between models using t-test.
* Report Steering Success Rate with 95% confidence intervals (Wilson score interval).
D) New Paper Outline
1. Introduction
   * The tension between "helpfulness" (social alignment) and "honesty" (factual alignment).
   * Shift from behavioral observation to mechanistic explanation.
   * Contribution: The "Turn Layer" and vector-based steering.
2. Related Work
   * LLM Social Behavior (Park 2023, Chuang 2024).
   * Mechanistic Interpretability: Geometry of Truth (Marks & Tegmark 2023), Sycophancy Steering (Rimsky 2023).
   * Differentiation: We apply this to dynamic social pressure (Asch paradigm), not static QA.
3. Methodology: The Abstract Agent Machine
   * Simulation engine (TraceDb, Olmo).
   * Probe construction (Dual-axis: Truth vs Social).
   * The "Turn Layer" definition.
4. Results: The Geometry of Compliance
   * Behavioral Baseline: Olmo-3 conformity rates (Instruct vs Think).
   * Mechanism: Visualization of the "Tug-of-War" (Truth vs Social projections).
   * The Turn: Identification of layers 20-24 as the decision point.
5. Results: Causal Intervention
   * Steering results: Subtracting social vector reduces conformity.
   * Comparison: Steering vs. Prompting ("Please be accurate").
6. Discussion: The "Think" Anomaly
   * How CoT models represent conflict differently (distributed vs discrete turn).
7. Limitations & Ethics
   * Probe validity (correlation vs causation).
   * Dual-use (inducing conformity).
8. Conclusion
E) Deletion List
* Delete: "OASIS Framework" references. Reason: Replaced by AAM/TraceDb.
* Delete: "Reddit-like environment" details (karma, ranking algorithms). Reason: Removing confounding variables to focus on internal state.
* Delete: Llama 3.1 / DeepSeek specific behavioral phenotypes. Reason: Not reproducible with current Olmo-based experiment artifacts.
* Delete: "Saturation at Score=1000" as a psychological finding. Reason: Likely an attention mechanism artifact; requires different analysis.
F) Draft Text
Abstract
Large Language Models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) often exhibit sycophancy, prioritizing perceived social consensus over factual accuracy. While behavioral studies have documented this "social conformity," its mechanistic origins remain opaque. We investigate the internal geometry of conformity using the Abstract Agent Machine (AAM) framework and the Olmo-3 model family. By training linear probes on "Truth" and "Social Consensus" axes, we reveal that conformity arises from a dynamic competition in the model's residual stream. We identify a critical "Turn Layer" (typically layers 20–24 in 7B models) where activation dominance shifts from factual representations to social compliance signals. Furthermore, we demonstrate that Causal Activation Steering—subtracting the "Social Vector" at these specific layers—significantly reduces conformity rates (by ~$X\%$), outperforming standard prompt-based mitigation. These findings suggest that social conformity is a structural feature of current alignment tuning, localized to mid-to-late transformer layers, and amenable to surgical intervention.
1. Introduction
As LLMs are integrated into social webs and collaborative agents, their ability to navigate the tension between honesty (factual correctness) and helpfulness (social alignment) becomes critical. Previous work [Chuang et al., 2024] suggested LLMs have a "Rational Accuracy Bias," yet empirically, models frequently succumb to sycophancy when presented with user errors or majority opinion [Panickssery et al., 2024].
This paper moves beyond behavioral observation to identifying the mechanistic drivers of this behavior. Is conformity a failure of knowledge retrieval, or an active suppression of known truth? Using the Olmo-3 open-weights family, we employ linear probing and activation steering to map the "Geometry of Compliance." We find that models often "know" the truth (high Truth Vector activation in early layers) but "decide" to conform (Social Vector dominance in late layers), effectively overwriting their own knowledge.
2. Methodology
We utilized the Abstract Agent Machine (AAM) to conduct reproducible "Synthetic Asch Experiments."
* Stimuli: The immutable_facts_minimal dataset (e.g., "The capital of France is Paris") coupled with social_conventions for training contrastive social probes.
* Probes: We trained two linear probes ($W_{truth}$, $W_{social}$) on the residual stream activations of allenai/Olmo-3-7B-Instruct layers 0–31. $W_{social}$ was trained on diverse consensus templates (e.g., "Everyone agrees...", "The majority says...") to ensure construct validity beyond simple n-gram detection.
* Metric: We define the Turn Layer ($L_{turn}$) as the first layer where the normalized projection of the Social Vector exceeds the Truth Vector: $\frac{v \cdot W_{soc}}{\|W_{soc}\|} > \frac{v \cdot W_{truth}}{\|W_{truth}\|}$.
3. Experiments & Results
* Experiment 1: Mapping the Turn. In control conditions, $P_{truth}$ rises monotonically through the network. In "Asch" conditions (5 confederates claiming incorrect facts), $P_{truth}$ rises initially but is suppressed by a sharp rise in $P_{soc}$ starting at Layer ~20.
* Experiment 2: Causal Steering. We intervened by subtracting the Social Vector ($v' = v - \alpha W_{soc}$) at the identified Turn Layers. This intervention recovered factual accuracy in 78% (placeholder) of conforming trials, confirming the causal role of the vector.
* Experiment 3: The 'Think' Difference. Comparing Olmo-3-7B-Instruct to Olmo-3-7B-Think reveals that Chain-of-Thought reasoning delays the Turn, allowing the model to surface factual knowledge in the reasoning trace before (potentially) succumbing to social pressure in the final token.
4. Limitations & Ethics
Our probes rely on supervised datasets which may not perfectly capture the model's internal ontology of "truth." Additionally, the ability to steer models away from conformity implies the ability to steer them toward deception, raising dual-use concerns.
G) Verification Log
* Verified: Olmo-3 models (Instruct/Think) exist and are supported by the provided suite_small.json.
* Verified: TraceDb schema supports storing conformity_probe_projections and activation_metadata.
* Verified: "Geometry of Truth" (Marks & Tegmark, 2023) and "Steering Llama 2" (Rimsky et al., 2023) are real, citable papers underpinning the vector analysis approach.
* Unverified: The exact "Turn Layer" number (20-24) is a hypothesis to be confirmed by the execution of the experiment; the text uses these as placeholders based on typical findings in 7B models.
* Unverified: "Rational Correction" behavior in Olmo-Think needs empirical confirmation from the asch_history_5 condition results.
H) Actionable TODO List
1. Refactor Probes (Human): Update experiments/olmo_conformity/datasets/prepare_social_probe.py to include at least 10 diverse templates for "consensus" to ensure validity.
2. Run Experiment (Agent): Execute aam run --suite experiments/olmo_conformity/configs/suite_small.json to generate real activation data.
3. Compute Projections (Agent): Run src/aam/experiments/olmo_conformity/vector_analysis.py to calculate $L_{turn}$ for Olmo-3.
4. Generate Plots (Agent): Create "Tug-of-War" line charts (Layer vs Projection Score) for the paper.
5. Finalize Text (Human): Replace placeholder stats (e.g., "78%") with actuals from the run.