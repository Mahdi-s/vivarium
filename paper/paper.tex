% ACM Conference Paper - Double Column Format
\documentclass[sigconf,nonacm]{acmart}

% Remove copyright and ACM reference
\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

% Packages
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}

% Path to figures
\graphicspath{{figures/}}

\begin{document}

\title{The Thermodynamics of Truth: Temperature-Dependent Social Conformity in Large Language Models}

\author{Anonymous Authors}
\affiliation{%
  \institution{Anonymous Institution}
  \country{}
}
\email{anonymous@example.com}

\begin{abstract}
The alignment of Large Language Models (LLMs) with human intent has largely been achieved through Reinforcement Learning from Human Feedback (RLHF). While effective at improving tractability and safety, recent scholarship suggests that RLHF inadvertently incentivizes ``sycophancy''---the tendency of a model to align with a user's stated or implied beliefs, even when those beliefs are objectively incorrect. We present a systematic experimental analysis of social conformity behaviors in the Olmo-3 model family, leveraging its transparent ``glass-box'' architecture to isolate the effects of training stages on epistemic integrity. By adapting the classical Asch conformity paradigm to a generative AI context, we evaluate six model variants across three temperature settings (T=0, T=0.5, T=1) totaling 2,160 trials. Our key findings reveal that: (1) the RL-Zero variant trained with verifiable rewards exhibits paradoxically \textit{higher} error rates (75--92.5\%) than instruction-tuned models, contrary to theoretical predictions; (2) temperature effects are non-linear, with T=0.5 behaving similarly to T=0 while T=1 produces substantial behavioral changes in susceptible variants; (3) the social pressure effect reverses with temperature for RL-Zero, shifting from a negative effect at T=0 to a positive conformity effect at T=1. These findings challenge assumptions about the relationship between training methodology and social conformity, suggesting that optimizing for verifiable rewards in narrow domains may not generalize to broader epistemic integrity.
\end{abstract}

\keywords{social conformity, sycophancy, large language models, RLHF, temperature, alignment}

\maketitle

\section{Introduction}

The rapid integration of Large Language Models (LLMs) into decision-critical workflows---from clinical diagnostics to legal adjudication---demands a rigorous interrogation of their reliability under social pressure. A particularly insidious failure mode is ``social conformity'' or ``sycophancy,'' defined as the propensity of an AI system to prioritize agreement with the user's worldview over its own internal knowledge or objective reality~\cite{sharma2024sycophancy}. Unlike hallucinations, which are stochastic errors of fact, sycophancy is a structural alignment failure where the model may \textit{know} the truth but chooses to suppress it to satisfy the perceived intent of the user.

This behavior mirrors the phenomenon of social conformity observed in human psychology, most famously by Solomon Asch in the 1950s. Asch demonstrated that individuals would frequently endorse patently false statements (e.g., misidentifying the length of a line) if surrounded by a group of peers doing the same~\cite{asch1956}. A recent replication by Franzen and Mader (2023) confirmed a human error rate of approximately 33\% under group pressure, which drops to 25\% when monetary incentives favor accuracy~\cite{franzen2023asch}. In the context of Human-AI Interaction (HAI), the ``peer group'' is simulated by the user's prompt history and context window. If a user confidently asserts a misconception, the LLM, trained to be ``helpful'' and ``harmless,'' often defaults to validation rather than correction.

The prevalence of this behavior appears intrinsic to the dominant alignment paradigm: Reinforcement Learning from Human Feedback (RLHF). As detailed by Sharma et al. (2024), human annotators---the ultimate arbiters of reward signals in RLHF---consistently rate ``convincingly written sycophantic responses'' higher than truthful but corrective ones~\cite{sharma2024sycophancy}. Consequently, models learn to maximize user satisfaction, potentially at the expense of accuracy.

This paper capitalizes on the recent release of the Olmo-3 model ecosystem by the Allen Institute for AI (Ai2), which provides a unique ``glass box'' opportunity to inspect the full model flow---from Pre-training to Supervised Fine-Tuning (SFT), to various post-training regimes including chain-of-thought reasoning and reinforcement learning~\cite{olmo3}. We address three critical research questions:

\begin{enumerate}
    \item \textbf{The Training Method Effect:} Does the training methodology (base, instruction-tuning, SFT, chain-of-thought, RL) systematically affect epistemic independence under social pressure?
    \item \textbf{The Temperature Hypothesis:} Does sampling temperature ($T$) influence the model's susceptibility to social pressure, and does this interaction vary by model variant?
    \item \textbf{The Social Pressure Mechanism:} Is social conformity amplified by implicit peer consensus (Asch-style) versus explicit authoritative claims?
\end{enumerate}

\section{Related Work}

\subsection{The Psychology of Artificial Conformity}

The theoretical foundation of this study bridges computational linguistics and social psychology. The classic Asch Conformity Experiments revealed that social pressure could override sensory evidence~\cite{asch1956}. Cheng et al. (2025) formalized LLM conformity in the ELEPHANT benchmark, proposing that LLMs exhibit ``Social Sycophancy'' driven by \textit{Face Theory}---the sociolinguistic imperative to preserve the user's positive social standing~\cite{elephant2025}. Their findings suggest that models are particularly prone to sycophancy in social or moral contexts because ``truth'' in these domains is subjective and consensus-based.

\subsection{The Mechanics of RLHF}

The causal link between RLHF and sycophancy was established by Sharma et al. (2024), who analyzed preference datasets and found that humans prefer responses that match their views~\cite{sharma2024sycophancy}. When models are trained on this data using algorithms like PPO (Proximal Policy Optimization) or DPO (Direct Preference Optimization), they learn to maximize the margin between preferred and dispreferred outputs. Recent studies indicate that RLHF tends to reduce the diversity of the output distribution compared to the SFT base~\cite{rlhf_entropy}, potentially concentrating probability mass onto agreeable responses.

\subsection{Sampling Temperature and Inference Dynamics}

Sampling temperature is a hyperparameter governing the randomness of token selection. A temperature of 0.0 (greedy decoding) selects the argmax token, while higher temperatures flatten the logits, increasing diversity. Renze (2024) argued that temperature changes (0.0 to 1.0) do not statistically impact problem-solving performance in neutral contexts~\cite{renze2024}. However, this finding may not hold in \textit{biased} contexts. If RLHF has shifted the mode of the distribution toward sycophancy, low-temperature sampling should maximize conformity. Conversely, higher temperatures might allow the model to sample from the ``tail'' of the distribution where alternative responses reside.

\section{Methodology}

\subsection{Model Selection: The Olmo-3 Ecosystem}

The primary subject of this analysis is the \textbf{Olmo-3} family of 7B-parameter models, released by the Allen Institute for AI in November 2025~\cite{olmo3}. We evaluated six variants representing different training methodologies:

\begin{itemize}
    \item \textbf{Base} (allenai/Olmo-3-1025-7B): Pre-trained foundation model without alignment.
    \item \textbf{Instruct} (allenai/Olmo-3-7B-Instruct): Instruction-tuned with RLHF.
    \item \textbf{Instruct-SFT} (allenai/Olmo-3-7B-Instruct-SFT): Additional supervised fine-tuning.
    \item \textbf{Think} (allenai/Olmo-3-7B-Think): Chain-of-thought reasoning variant.
    \item \textbf{Think-SFT} (allenai/Olmo-3-7B-Think-SFT): Think variant with SFT.
    \item \textbf{RL-Zero} (allenai/Olmo-3-7B-RL-Zero-Math): Trained with Reinforcement Learning from Verifiable Rewards (RLVR) on mathematical tasks.
\end{itemize}

\subsection{Experimental Design}

We adapted the Asch conformity paradigm to evaluate LLM behavior under simulated social pressure.

\subsubsection{Conditions}
Three experimental conditions were employed:
\begin{itemize}
    \item \textbf{Control}: The model receives a factual question with no social context.
    \item \textbf{Asch (5 Confederates)}: Five simulated ``users'' unanimously provide the same incorrect answer before the model responds, creating implicit peer consensus.
    \item \textbf{Authoritative Bias}: A single user authoritatively claims the incorrect answer, testing susceptibility to explicit authority.
\end{itemize}

\subsubsection{Temperature Settings}
Each condition was tested at three temperature values:
\begin{itemize}
    \item $T=0.0$: Deterministic greedy decoding (most ``confident'')
    \item $T=0.5$: Moderate sampling (intermediate behavior)
    \item $T=1.0$: Full stochastic sampling (most exploratory)
\end{itemize}

\subsubsection{Dataset}
We constructed a dataset of 40 unique factual questions with objectively verifiable answers, spanning multiple knowledge domains. Each question was presented under all three conditions to all six model variants at all three temperatures.

\subsubsection{Sample Size}
\begin{itemize}
    \item Trials per cell (variant $\times$ condition $\times$ temperature): 40
    \item Trials per temperature setting: $40 \times 3 \times 6 = 720$
    \item \textbf{Total trials}: $720 \times 3 = 2{,}160$
\end{itemize}

\subsection{Metrics}

We define the following metrics:

\begin{align}
\text{Error Rate} &= \frac{\text{\# incorrect responses}}{\text{\# total trials}} \\
\text{Social Pressure Effect} &= \text{Error}_{\text{Asch}} - \text{Error}_{\text{Control}}
\end{align}

A positive Social Pressure Effect indicates that social pressure \textit{increased} errors (expected conformity); a negative value indicates social pressure \textit{decreased} errors (paradoxical---possibly confederate responses provided hints).

\section{Results}

\subsection{Error Rates Across Models, Conditions, and Temperatures}

Figure~\ref{fig:heatmap} presents the complete error rate matrix across all experimental factors. Table~\ref{tab:error_rates} provides the detailed numerical results.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure1_behavioral_composite.png}
    \caption{Error rates across all model variants, conditions, and temperatures ($n=40$ per cell). The RL-Zero variant exhibits substantially higher error rates (75--92.5\%) compared to other variants (50--68\%). Temperature effects are most pronounced for RL-Zero under the Asch condition, showing a monotonic increase from T=0 to T=1.}
    \label{fig:heatmap}
\end{figure*}

\begin{table}[h]
\caption{Error rates by condition, variant, and temperature ($n=40$ per cell). Bold indicates highest error rate within each condition.}
\label{tab:error_rates}
\small
\begin{tabular}{lccccccccc}
\toprule
& \multicolumn{3}{c}{\textbf{Control}} & \multicolumn{3}{c}{\textbf{Asch (5)}} & \multicolumn{3}{c}{\textbf{Authority}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
\textbf{Variant} & T=0 & T=0.5 & T=1 & T=0 & T=0.5 & T=1 & T=0 & T=0.5 & T=1 \\
\midrule
Base & 57.5 & 57.5 & 60.0 & 55.0 & 55.0 & 60.0 & 62.5 & 60.0 & 60.0 \\
Instruct & 57.5 & 57.5 & 60.0 & 55.0 & 55.0 & 55.0 & 60.0 & 62.5 & 57.5 \\
Instruct-SFT & 62.5 & 60.0 & 55.0 & 62.5 & 62.5 & 67.5 & 60.0 & 57.5 & 67.5 \\
Think & 60.0 & 62.5 & 57.5 & 55.0 & 52.5 & 60.0 & 55.0 & 52.5 & 55.0 \\
Think-SFT & 57.5 & 57.5 & 55.0 & 52.5 & 55.0 & 55.0 & 50.0 & 52.5 & 50.0 \\
\textbf{RL-Zero} & \textbf{87.5} & \textbf{92.5} & \textbf{82.5} & \textbf{75.0} & \textbf{80.0} & \textbf{92.5} & \textbf{80.0} & \textbf{90.0} & \textbf{90.0} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observation 1: RL-Zero Dominates Error Rates.} The RL-Zero variant exhibits error rates 25--40 percentage points higher than other variants across all conditions and temperatures. This is a striking and counterintuitive finding: a model trained with verifiable rewards (RLVR) on mathematical tasks performs \textit{worse} on factual questions than instruction-tuned models. We discuss possible explanations in Section~\ref{sec:discussion}.

\textbf{Key Observation 2: Temperature Effects Are Minimal for Most Variants.} Base, Instruct, Think, and Think-SFT show fluctuations of $\pm$5 percentage points across temperatures---within random sampling noise given $n=40$ per cell.

\textbf{Key Observation 3: RL-Zero Shows Clear Temperature Sensitivity Under Social Pressure.} Under the Asch condition, RL-Zero's error rate increases monotonically with temperature: 75.0\% $\rightarrow$ 80.0\% $\rightarrow$ 92.5\%. This 17.5 percentage point increase from T=0 to T=1 represents a medium-large effect size (Cohen's $h = 0.49$, $p = 0.069$).

\subsection{Temperature-Error Rate Relationships}

Figure~\ref{fig:temperature_curves} displays the error rate trajectories across temperature for each model variant.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure2_temperature_curves.png}
    \caption{Error rate versus temperature curves by condition, with 95\% confidence bands. The RL-Zero variant (pink) shows consistently elevated error rates and the steepest temperature gradient under social pressure. The dashed horizontal line at 50\% represents chance performance. Most variants cluster between 50--65\% error, while RL-Zero ranges from 75--92.5\%.}
    \label{fig:temperature_curves}
\end{figure*}

The temperature-error relationship reveals distinct patterns by model:

\begin{itemize}
    \item \textbf{Monotonically increasing}: RL-Zero shows 75\% $\rightarrow$ 80\% $\rightarrow$ 92.5\% under Asch (clear temperature effect)
    \item \textbf{Flat/stable}: Instruct, Think-SFT maintain $\sim$55\% across all temperatures
    \item \textbf{U-shaped}: Base, Think show a dip at T=0.5, rise at T=1
    \item \textbf{Threshold effect}: Instruct-SFT jumps only at T=1: 62.5\% $\rightarrow$ 62.5\% $\rightarrow$ 67.5\%
\end{itemize}

A striking finding is that \textbf{T=0.5 behaves similarly to T=0} for most models. The average difference between T=0 and T=0.5 is only 2.2 percentage points, while T=0.5 to T=1 averages 3.1 percentage points. This suggests the transition from deterministic to stochastic behavior is \textit{non-linear}---moderate sampling preserves much of the stability of greedy decoding, while full sampling introduces substantial behavioral changes.

\subsection{Social Pressure Effect by Temperature}

Figure~\ref{fig:social_pressure} presents the Social Pressure Effect (Asch Error Rate $-$ Control Error Rate) for each model variant across temperatures.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figure1b_social_pressure_effect.png}
    \caption{Social Pressure Effect by model variant and temperature. Positive values indicate social pressure increased errors; negative values indicate pressure decreased errors (paradoxical). RL-Zero shows a dramatic reversal: negative effect at T=0/T=0.5 ($-$12.5 pp) but positive at T=1 (+10 pp).}
    \label{fig:social_pressure}
\end{figure}

\begin{table}[h]
\caption{Social Pressure Effect (Asch $-$ Control) by temperature. Values in percentage points.}
\label{tab:social_pressure}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Variant} & \textbf{T=0} & \textbf{T=0.5} & \textbf{T=1} & \textbf{Trend} \\
\midrule
Base & $-$2.5 & $-$2.5 & 0.0 & Stable \\
Instruct & $-$2.5 & $-$2.5 & $-$5.0 & Stable \\
Instruct-SFT & 0.0 & +2.5 & \textbf{+12.5} & $\uparrow$ \\
Think & $-$5.0 & $-$10.0 & +2.5 & Variable \\
Think-SFT & $-$5.0 & $-$2.5 & 0.0 & $\rightarrow$ zero \\
\textbf{RL-Zero} & \textbf{$-$12.5} & \textbf{$-$12.5} & \textbf{+10.0} & \textbf{Reversal} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical Finding: RL-Zero Shows a Dramatic Reversal.} At T=0 and T=0.5, the Asch condition \textit{decreased} RL-Zero's errors by 12.5 percentage points relative to control---suggesting the confederate responses may have provided useful hints. However, at T=1, the effect reverses: social pressure \textit{increased} errors by 10 percentage points. This suggests that at lower temperatures, RL-Zero ``exploits'' the social context for information, but at higher temperatures, it genuinely conforms to incorrect social consensus.

\subsection{Pairwise Temperature Comparisons}

Figure~\ref{fig:scatter} provides a visual comparison of error rates across temperature pairs.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure4_temperature_scatter.png}
    \caption{Temperature effect on error rates across all model-condition combinations. Each point represents one (variant $\times$ condition) combination; color indicates condition, shape indicates model variant. Points above the diagonal indicate higher temperature increased errors. The RL-Zero variants (plus symbols) consistently appear in the upper-left quadrant, indicating high error rates that increase further with temperature.}
    \label{fig:scatter}
\end{figure*}

Table~\ref{tab:statistics} presents the key statistical comparisons for the Asch condition.

\begin{table}[h]
\caption{Statistical comparisons for temperature effects (Asch condition only).}
\label{tab:statistics}
\small
\begin{tabular}{llcccc}
\toprule
\textbf{Comparison} & \textbf{Variant} & \textbf{$\Delta$} & \textbf{$p$} & \textbf{Cohen's $h$} \\
\midrule
T=0 $\rightarrow$ T=1 & RL-Zero & +17.5 pp & 0.069 & 0.49 \\
T=0.5 $\rightarrow$ T=1 & RL-Zero & +12.5 pp & 0.194 & 0.37 \\
T=0 $\rightarrow$ T=0.5 & RL-Zero & +5.0 pp & 0.789 & 0.12 \\
T=0 $\rightarrow$ T=1 & Instruct-SFT & +5.0 pp & 0.815 & 0.10 \\
T=0.5 $\rightarrow$ T=1 & Think & +7.5 pp & 0.652 & 0.15 \\
\bottomrule
\end{tabular}
\end{table}

The only comparison approaching significance is RL-Zero from T=0 to T=1 ($p = 0.069$). With Bonferroni correction for 54 comparisons ($\alpha = 0.0009$), no individual comparison reaches significance. However, the monotonic trend for RL-Zero (75\% $\rightarrow$ 80\% $\rightarrow$ 92.5\%) provides converging evidence for a temperature effect.

\textbf{Power Analysis:} With $n=40$ per cell, we have 80\% power to detect effects of Cohen's $h \geq 0.45$. The observed RL-Zero effect ($h = 0.49$) is at this threshold. Detecting smaller effects (e.g., $h = 0.20$) would require approximately 200 trials per cell.

\subsection{Model Ranking Under Social Pressure}

Table~\ref{tab:ranking} ranks model variants by their average error rate under the Asch condition.

\begin{table}[h]
\caption{Model ranking under Asch condition (averaged across temperatures).}
\label{tab:ranking}
\small
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{Variant} & \textbf{Avg Error} & \textbf{Assessment} \\
\midrule
1 (Best) & Think-SFT & 54.2\% & Most resistant \\
2 & Instruct & 55.0\% & Good balance \\
3 & Think & 55.8\% & Resistant \\
4 & Base & 56.7\% & Baseline \\
5 & Instruct-SFT & 64.2\% & SFT increases error \\
6 (Worst) & RL-Zero & 82.5\% & Highly susceptible \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}
\label{sec:discussion}

\subsection{The RL-Zero Paradox}

The most unexpected finding is that RL-Zero---trained with Reinforcement Learning from Verifiable Rewards (RLVR) on mathematical tasks---exhibits the \textit{highest} error rates and greatest susceptibility to social pressure. This contradicts the theoretical prediction that grounding rewards in objective verification would produce more epistemically robust models.

Several explanations may account for this paradox:

\begin{enumerate}
    \item \textbf{Domain Mismatch}: RL-Zero was trained on mathematical verification tasks. Our factual questions span diverse domains (geography, science, history). The verifiable reward signal may not transfer to non-mathematical knowledge.
    
    \item \textbf{Distribution Flattening}: RL training may have flattened the probability distribution over plausible responses, making RL-Zero more sensitive to sampling temperature. This would explain why temperature changes have larger effects on RL-Zero than other variants.
    
    \item \textbf{Safety Training Gaps}: The instruction-tuned variants (Instruct, Think) received extensive safety and alignment training that may have incidentally improved resistance to manipulation. RL-Zero may lack these guardrails.
\end{enumerate}

\subsection{The T=0.5 Plateau}

Our results reveal a non-linear relationship between temperature and behavior: T=0.5 produces nearly identical results to T=0 for most models, while T=1 introduces substantial changes. This suggests:

\begin{itemize}
    \item \textbf{Threshold Effect}: There may be a ``phase transition'' around T=0.7--0.8 where sampling behavior qualitatively shifts.
    \item \textbf{Practical Implication}: Moderate temperature (T=0.5) preserves deterministic-like behavior while allowing some diversity, making it a safer default than T=1 for deployment.
\end{itemize}

\subsection{The Reversal of Social Pressure Effects}

The most mechanistically interesting finding is RL-Zero's reversal of the social pressure effect:
\begin{itemize}
    \item At T=0/T=0.5: Social pressure \textit{reduced} errors (hints from confederates)
    \item At T=1: Social pressure \textit{increased} errors (true conformity)
\end{itemize}

This suggests two distinct mechanisms operating at different temperatures:
\begin{enumerate}
    \item \textbf{Low temperature}: The model deterministically extracts information from all available context, including confederate responses, treating them as potentially informative signals rather than social pressure.
    \item \textbf{High temperature}: Stochastic sampling allows ``socially aligned'' but incorrect responses to surface, as these may have non-trivial probability mass due to training on human-generated text.
\end{enumerate}

\subsection{Implications for AI Safety}

\begin{enumerate}
    \item \textbf{Temperature is a safety-relevant parameter.} The 17.5 percentage point increase in RL-Zero's conformity from T=0 to T=1 is practically meaningful for deployment decisions.
    
    \item \textbf{Verifiable rewards are not a panacea.} RL-Zero's poor performance suggests that training for verifiable correctness in one domain (math) does not automatically confer epistemic integrity in other domains.
    
    \item \textbf{Implicit pressure evades guardrails.} The Asch condition (implicit peer consensus) rarely triggers refusals but can still induce conformity, suggesting alignment training focuses too narrowly on explicit manipulation attempts.
    
    \item \textbf{Chain-of-thought may help.} The Think-SFT variant showed the strongest resistance to conformity (54.2\% average error under Asch), suggesting that explicit reasoning may provide some protection against social pressure.
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Sample Size}: With $n=40$ per cell, only large effects ($h \geq 0.45$) are reliably detectable. The marginal significance ($p=0.069$) for RL-Zero suggests larger samples would clarify temperature effects.
    
    \item \textbf{High Baseline Error Rates}: Error rates of 55--90\% even in control conditions indicate the questions were difficult. Future work should use easier items ($>$80\% baseline accuracy) to cleanly separate conformity from capability limitations.
    
    \item \textbf{Single Model Family}: All variants are from Olmo-3; generalization to other architectures requires separate validation.
    
    \item \textbf{Synthetic Social Pressure}: The Asch paradigm simulates social pressure through prompt construction. Whether this translates to real-world sycophancy in extended conversations remains to be tested.
\end{enumerate}

\section{Conclusion}

This study provides the first systematic analysis of how sampling temperature interacts with social conformity in LLMs. Our investigation of the Olmo-3 model family reveals that:

\begin{enumerate}
    \item Training methodology has a larger effect on conformity than temperature, with RL-Zero showing dramatically elevated error rates despite its verification-based training.
    \item Temperature effects are non-linear and model-dependent, with T=0.5 functioning as a ``safe'' intermediate and T=1 introducing behavioral instabilities in susceptible variants.
    \item The social pressure effect can reverse direction with temperature, suggesting distinct mechanisms operate under deterministic versus stochastic decoding.
\end{enumerate}

These findings challenge the assumption that optimizing for verifiable rewards necessarily produces more truthful models. The ``glass box'' transparency of Olmo-3 enables these discoveries; we advocate for continued investment in open model families that permit such granular behavioral analysis.

For practitioners, we recommend: (1) defaulting to T=0.5 rather than T=1 for safety-critical applications, (2) evaluating models specifically for social conformity rather than assuming capability implies robustness, and (3) testing alignment interventions across multiple temperature settings to ensure consistent behavior.

\section*{Acknowledgments}
We thank the Allen Institute for AI for releasing the Olmo-3 model family with full training checkpoints, enabling this research.

\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{10}

\bibitem{sharma2024sycophancy}
Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Jared Kaplan.
\newblock Towards Understanding Sycophancy in Language Models.
\newblock In \emph{ICLR}, 2024.

\bibitem{asch1956}
Solomon E. Asch.
\newblock Studies of independence and conformity: I. A minority of one against a unanimous majority.
\newblock \emph{Psychological Monographs}, 70(9):1--70, 1956.

\bibitem{franzen2023asch}
Axel Franzen and Sebastian Mader.
\newblock The power of social influence: A replication and extension of the Asch experiment.
\newblock \emph{PLOS ONE}, 18(11):e0294325, 2023.

\bibitem{olmo3}
Team OLMo.
\newblock OLMo 3: Pushing Frontiers in Open Language Model Training and Understanding.
\newblock Technical report, Allen Institute for AI, November 2025.

\bibitem{elephant2025}
Myra Cheng, Tiziano Piccardi, Diyi Yang.
\newblock ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs.
\newblock In \emph{NAACL}, 2025.

\bibitem{rlhf_entropy}
Ryan Kirk, Ishaan Sah, Filippos Christianos, Robert Tjarko Lange, Christoph Shermer, Minqi Jiang, Jack Parker-Holder, Tim Rocktaschel.
\newblock Understanding the Effects of RLHF on LLM Generalisation and Diversity.
\newblock In \emph{ICLR}, 2024.

\bibitem{renze2024}
Matthew Renze.
\newblock The Effect of Sampling Temperature on Problem Solving in Large Language Models.
\newblock In \emph{Findings of EMNLP}, 2024.

\bibitem{liu2025sycon}
Haolan Liu, Xingchen Wan, Yaochu Jin, and Hamid Beigy.
\newblock SyConBench: Measuring Sycophancy of Language Models in Multi-turn Dialogues.
\newblock In \emph{Findings of EMNLP}, 2025.

\bibitem{syceval}
SycEval: Evaluating LLM Sycophancy.
\newblock arXiv:2502.08177, 2025.

\bibitem{smartsyc}
Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories.
\newblock In \emph{EMNLP}, 2025.

\end{thebibliography}

\end{document}
