{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Run Analysis Notebook\n",
    "\n",
    "This notebook provides a complete analysis of any Olmo Conformity experiment run.\n",
    "\n",
    "**Features:**\n",
    "- Run metadata and configuration\n",
    "- Behavioral trial analysis with examples\n",
    "- Activation analysis from safetensors files\n",
    "- Probe projection analysis\n",
    "- Logit lens analysis\n",
    "- Think token analysis (for Think variants)\n",
    "- Intervention results\n",
    "- Sample prompts and responses\n",
    "\n",
    "**Usage:**\n",
    "1. Set `RUN_ID` in the cell below\n",
    "2. Run all cells to get comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Set Your Run ID\n",
    "\n",
    "**Run this cell first.** Set `RUN_ID` to match your run. You can use:\n",
    "- Full directory name: `20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631`\n",
    "- Just the UUID: `0af03fbc-d576-4afa-9815-b37a11f57631`\n",
    "- Or set `RUN_DIR` to the full path to the run folder\n",
    "\n",
    "Then run all remaining cells for full analysis (DB, Safetensors, prompts, responses, probes, interventions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631\n",
      "Runs directory: ./runs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Set your run ID here\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Provide full run ID (timestamp_UUID format)\n",
    "RUN_ID = \"20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631\"\n",
    "\n",
    "# Option 2: Or provide just the UUID part (will search for matching run directory)\n",
    "# RUN_ID = \"0af03fbc-d576-4afa-9815-b37a11f57631\"\n",
    "\n",
    "# Option 3: Or provide run directory path directly\n",
    "# RUN_DIR = \"/path/to/runs/20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631\"\n",
    "\n",
    "# Base runs directory (default: ./runs)\n",
    "RUNS_DIR = \"./runs\"\n",
    "\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Runs directory: {RUNS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "   REPO_ROOT: /Users/mahdi/repos/abstractAgentMachine\n",
      "   RUNS_DIR: /Users/mahdi/repos/abstractAgentMachine/runs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Imports and utilities\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add repo root and src to path for imports\n",
    "REPO_ROOT = Path.cwd().parent if (Path.cwd() / \"utils.py\").exists() or Path.cwd().name == \"Analysis Scripts\" else Path.cwd()\n",
    "if (REPO_ROOT / \"src\" / \"aam\").exists():\n",
    "    pass  # REPO_ROOT is correct\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd()\n",
    "SRC_ROOT = REPO_ROOT / \"src\"\n",
    "if str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_ROOT))\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "# Import AAM modules\n",
    "from aam.persistence import TraceDb, TraceDbConfig\n",
    "from aam.analytics.utils import load_simulation_db, check_missing_prerequisites\n",
    "\n",
    "# Import utils for run resolution (from Analysis Scripts folder)\n",
    "sys.path.insert(0, str(REPO_ROOT / \"Analysis Scripts\"))\n",
    "from utils import find_run_dir_for_run_id, resolve_run_ref\n",
    "\n",
    "# Safetensors for activation loading\n",
    "try:\n",
    "    from safetensors.torch import load_file as load_safetensors\n",
    "    import torch\n",
    "    SAFETENSORS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SAFETENSORS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  safetensors not installed - activation analysis will be limited\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Resolve RUNS_DIR relative to repo root if it looks relative\n",
    "if not os.path.isabs(RUNS_DIR):\n",
    "    RUNS_DIR = str(REPO_ROOT / \"runs\")\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"   REPO_ROOT: {REPO_ROOT}\")\n",
    "print(f\"   RUNS_DIR: {RUNS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find and Load Run Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found run directory: /Users/mahdi/repos/abstractAgentMachine/runs/20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631\n",
      "‚úÖ Run ID: 20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631\n",
      "\n",
      "üìÅ Paths:\n",
      "   Database: /Users/mahdi/repos/abstractAgentMachine/runs/20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631/simulation.db (exists: True)\n",
      "   Activations: /Users/mahdi/repos/abstractAgentMachine/runs/20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631/activations (exists: True)\n",
      "   Artifacts: /Users/mahdi/repos/abstractAgentMachine/runs/20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631/artifacts (exists: True)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Find and load run directory\n",
    "# ============================================================================\n",
    "\n",
    "# Resolve run directory\n",
    "try:\n",
    "    if 'RUN_DIR' in globals():\n",
    "        run_ref = resolve_run_ref(run_id=None, run_dir=RUN_DIR, runs_dir=RUNS_DIR)\n",
    "    else:\n",
    "        run_ref = resolve_run_ref(run_id=RUN_ID, run_dir=None, runs_dir=RUNS_DIR)\n",
    "    \n",
    "    RUN_DIR = run_ref.run_dir\n",
    "    RUN_ID = run_ref.run_id\n",
    "    \n",
    "    print(f\"‚úÖ Found run directory: {RUN_DIR}\")\n",
    "    print(f\"‚úÖ Run ID: {RUN_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error finding run: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify paths\n",
    "DB_PATH = Path(RUN_DIR) / \"simulation.db\"\n",
    "ACTIVATIONS_DIR = Path(RUN_DIR) / \"activations\"\n",
    "ARTIFACTS_DIR = Path(RUN_DIR) / \"artifacts\"\n",
    "\n",
    "print(f\"\\nüìÅ Paths:\")\n",
    "print(f\"   Database: {DB_PATH} (exists: {DB_PATH.exists()})\")\n",
    "print(f\"   Activations: {ACTIVATIONS_DIR} (exists: {ACTIVATIONS_DIR.exists()})\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR} (exists: {ARTIFACTS_DIR.exists()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Database and Check Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database loaded\n",
      "\n",
      "üìä Available data:\n",
      "   ‚ùå judgeval_scores: False\n",
      "   ‚ùå probes: False\n",
      "   ‚ùå probe_projections: False\n",
      "   ‚ùå interventions: False\n",
      "   ‚ùå intervention_results: False\n",
      "   ‚ùå think_tokens: False\n",
      "   ‚ùå logit_lens: False\n",
      "   ‚ùå activation_capture: False\n",
      "‚ùå Run not found in database\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Run 20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Run not found in database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Run 20260124_230102_0af03fbc-d576-4afa-9815-b37a11f57631 not found"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load database and check prerequisites\n",
    "# ============================================================================\n",
    "\n",
    "# Load database\n",
    "trace_db = load_simulation_db(RUN_DIR)\n",
    "print(\"‚úÖ Database loaded\")\n",
    "\n",
    "# Check what data is available\n",
    "prerequisites = check_missing_prerequisites(trace_db, RUN_ID)\n",
    "print(\"\\nüìä Available data:\")\n",
    "for name, exists in prerequisites.items():\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {name}: {exists}\")\n",
    "\n",
    "# Get run metadata\n",
    "run_meta = trace_db.conn.execute(\n",
    "    \"SELECT run_id, seed, created_at, config_json FROM runs WHERE run_id = ?\",\n",
    "    (RUN_ID,)\n",
    ").fetchone()\n",
    "\n",
    "if run_meta:\n",
    "    config = json.loads(run_meta[\"config_json\"])\n",
    "    print(f\"\\nüìã Run Metadata:\")\n",
    "    print(f\"   Run ID: {run_meta['run_id']}\")\n",
    "    print(f\"   Seed: {run_meta['seed']}\")\n",
    "    print(f\"   Created: {datetime.fromtimestamp(run_meta['created_at'])}\")\n",
    "    print(f\"   Config mode: {config.get('mode', 'unknown')}\")\n",
    "    if 'suite_config' in config:\n",
    "        suite_cfg = config['suite_config']\n",
    "        print(f\"   Temperature: {suite_cfg.get('run', {}).get('temperature', 'unknown')}\")\n",
    "        print(f\"   Models: {len(suite_cfg.get('models', []))}\")\n",
    "        print(f\"   Conditions: {len(suite_cfg.get('conditions', []))}\")\n",
    "else:\n",
    "    print(\"‚ùå Run not found in database\")\n",
    "    raise ValueError(f\"Run {RUN_ID} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Run Overview and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Run Statistics Overview\n",
    "# ============================================================================\n",
    "\n",
    "# Count trials by variant and condition\n",
    "trials_summary = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        t.variant,\n",
    "        c.name as condition_name,\n",
    "        COUNT(*) as trial_count,\n",
    "        SUM(CASE WHEN o.is_correct = 1 THEN 1 ELSE 0 END) as correct_count,\n",
    "        SUM(CASE WHEN o.is_correct = 0 THEN 1 ELSE 0 END) as incorrect_count,\n",
    "        SUM(CASE WHEN o.is_correct IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "        AVG(CASE WHEN o.is_correct = 1 THEN 1.0 ELSE 0.0 END) as accuracy\n",
    "    FROM conformity_trials t\n",
    "    LEFT JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "    LEFT JOIN conformity_outputs o ON o.trial_id = t.trial_id\n",
    "    WHERE t.run_id = ?\n",
    "    GROUP BY t.variant, c.name\n",
    "    ORDER BY t.variant, c.name\n",
    "\"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "\n",
    "print(\"üìä Trial Summary by Variant and Condition:\")\n",
    "print(trials_summary.to_string(index=False))\n",
    "\n",
    "# Overall statistics\n",
    "total_trials = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) as total FROM conformity_trials WHERE run_id = ?\n",
    "\"\"\", trace_db.conn, params=(RUN_ID,)).iloc[0]['total']\n",
    "\n",
    "total_outputs = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) as total \n",
    "    FROM conformity_outputs o\n",
    "    JOIN conformity_trials t ON t.trial_id = o.trial_id\n",
    "    WHERE t.run_id = ?\n",
    "\"\"\", trace_db.conn, params=(RUN_ID,)).iloc[0]['total']\n",
    "\n",
    "print(f\"\\nüìà Overall Statistics:\")\n",
    "print(f\"   Total trials: {total_trials}\")\n",
    "print(f\"   Total outputs: {total_outputs}\")\n",
    "print(f\"   Variants tested: {trials_summary['variant'].nunique()}\")\n",
    "print(f\"   Conditions tested: {trials_summary['condition_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Sample Prompts and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Extract Sample Prompts and Responses\n",
    "# ============================================================================\n",
    "\n",
    "# Get sample trials with full prompt and response data\n",
    "samples = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        t.trial_id,\n",
    "        t.variant,\n",
    "        t.model_id,\n",
    "        t.temperature,\n",
    "        c.name as condition_name,\n",
    "        i.question,\n",
    "        i.ground_truth_text,\n",
    "        i.domain,\n",
    "        p.system_prompt,\n",
    "        p.user_prompt,\n",
    "        p.chat_history_json,\n",
    "        o.raw_text as response,\n",
    "        o.parsed_answer_text,\n",
    "        o.is_correct,\n",
    "        o.refusal_flag,\n",
    "        o.latency_ms,\n",
    "        o.parsed_answer_json\n",
    "    FROM conformity_trials t\n",
    "    JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "    JOIN conformity_items i ON i.item_id = t.item_id\n",
    "    LEFT JOIN conformity_prompts p ON p.trial_id = t.trial_id\n",
    "    LEFT JOIN conformity_outputs o ON o.trial_id = t.trial_id\n",
    "    WHERE t.run_id = ?\n",
    "    ORDER BY t.created_at\n",
    "    LIMIT 50\n",
    "\"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "\n",
    "print(f\"üìù Loaded {len(samples)} sample trials\")\n",
    "\n",
    "# Display samples by condition\n",
    "for condition in samples['condition_name'].unique():\n",
    "    if pd.isna(condition):\n",
    "        continue\n",
    "    cond_samples = samples[samples['condition_name'] == condition].head(3)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìã Condition: {condition}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for idx, row in cond_samples.iterrows():\n",
    "        print(f\"\\n--- Sample {idx + 1} ---\")\n",
    "        print(f\"Variant: {row['variant']} | Domain: {row['domain']} | Temp: {row['temperature']}\")\n",
    "        print(f\"\\nQuestion: {row['question']}\")\n",
    "        print(f\"Ground Truth: {row['ground_truth_text']}\")\n",
    "        print(f\"\\nSystem Prompt:\")\n",
    "        print(f\"  {row['system_prompt']}\")\n",
    "        print(f\"\\nUser Prompt:\")\n",
    "        print(f\"  {row['user_prompt']}\")\n",
    "        print(f\"\\nResponse:\")\n",
    "        print(f\"  {row['response']}\")\n",
    "        print(f\"\\nParsed: {row['parsed_answer_text']}\")\n",
    "        print(f\"Correct: {row['is_correct']} | Refusal: {row['refusal_flag']} | Latency: {row['latency_ms']:.1f}ms\")\n",
    "        \n",
    "        if row['parsed_answer_json']:\n",
    "            judgeval = json.loads(row['parsed_answer_json'])\n",
    "            print(f\"Judge Eval: {judgeval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Activation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Activation Metadata and File Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('activation_capture', False):\n",
    "    # Get activation metadata\n",
    "    activation_meta = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            layer_index,\n",
    "            component,\n",
    "            COUNT(*) as count,\n",
    "            MIN(created_at) as first_capture,\n",
    "            MAX(created_at) as last_capture\n",
    "        FROM activation_metadata\n",
    "        WHERE run_id = ?\n",
    "        GROUP BY layer_index, component\n",
    "        ORDER BY layer_index, component\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"üî¨ Activation Metadata Summary:\")\n",
    "    print(activation_meta.to_string(index=False))\n",
    "    \n",
    "    # List activation files\n",
    "    if ACTIVATIONS_DIR.exists():\n",
    "        activation_files = sorted(ACTIVATIONS_DIR.glob(\"*.safetensors\"))\n",
    "        print(f\"\\nüìÅ Found {len(activation_files)} activation file(s)\")\n",
    "        \n",
    "        if activation_files and SAFETENSORS_AVAILABLE:\n",
    "            # Load a sample file to inspect structure\n",
    "            sample_file = activation_files[0]\n",
    "            print(f\"\\nüìÇ Sample file: {sample_file.name}\")\n",
    "            \n",
    "            try:\n",
    "                tensors = load_safetensors(str(sample_file))\n",
    "                keys = list(tensors.keys())\n",
    "                print(f\"   Tensor keys: {len(keys)}\")\n",
    "                print(f\"   First 10 keys: {keys[:10]}\")\n",
    "                \n",
    "                # Show tensor shapes\n",
    "                print(f\"\\n   Tensor shapes (first 5):\")\n",
    "                for key in keys[:5]:\n",
    "                    tensor = tensors[key]\n",
    "                    if isinstance(tensor, torch.Tensor):\n",
    "                        print(f\"     {key}: {tensor.shape} ({tensor.dtype})\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error loading file: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå Activations directory not found\")\n",
    "else:\n",
    "    print(\"‚ùå No activation capture data available for this run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load Sample Activations for Specific Trial\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('activation_capture', False) and SAFETENSORS_AVAILABLE:\n",
    "    # Get a sample trial with activations\n",
    "    trial_with_activations = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            t.trial_id,\n",
    "            t.variant,\n",
    "            s.time_step,\n",
    "            s.agent_id,\n",
    "            c.name as condition_name,\n",
    "            i.question\n",
    "        FROM conformity_trials t\n",
    "        JOIN conformity_trial_steps s ON s.trial_id = t.trial_id\n",
    "        JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "        JOIN conformity_items i ON i.item_id = t.item_id\n",
    "        WHERE t.run_id = ?\n",
    "        LIMIT 1\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    if len(trial_with_activations) > 0:\n",
    "        sample_trial = trial_with_activations.iloc[0]\n",
    "        trial_id = sample_trial['trial_id']\n",
    "        time_step = sample_trial['time_step']\n",
    "        agent_id = sample_trial['agent_id']\n",
    "        \n",
    "        print(f\"üìä Sample Trial: {trial_id[:8]}...\")\n",
    "        print(f\"   Variant: {sample_trial['variant']}\")\n",
    "        print(f\"   Condition: {sample_trial['condition_name']}\")\n",
    "        print(f\"   Question: {sample_trial['question']}\")\n",
    "        print(f\"   Time Step: {time_step}, Agent: {agent_id}\")\n",
    "        \n",
    "        # Get activation records for this trial\n",
    "        activations = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                layer_index,\n",
    "                component,\n",
    "                shard_file_path,\n",
    "                tensor_key,\n",
    "                shape_json,\n",
    "                dtype\n",
    "            FROM activation_metadata\n",
    "            WHERE run_id = ? AND time_step = ? AND agent_id = ?\n",
    "            ORDER BY layer_index\n",
    "        \"\"\", trace_db.conn, params=(RUN_ID, time_step, agent_id))\n",
    "        \n",
    "        print(f\"\\nüî¨ Found {len(activations)} activation records\")\n",
    "        \n",
    "        # Load activations for a few layers\n",
    "        if len(activations) > 0:\n",
    "            print(f\"\\nüìà Sample Activation Vectors:\")\n",
    "            for idx, act_row in activations.head(5).iterrows():\n",
    "                shard_path = Path(RUN_DIR) / act_row['shard_file_path']\n",
    "                if shard_path.exists():\n",
    "                    try:\n",
    "                        tensors = load_safetensors(str(shard_path))\n",
    "                        vec = tensors[act_row['tensor_key']]\n",
    "                        if isinstance(vec, torch.Tensor):\n",
    "                            print(f\"   Layer {act_row['layer_index']} ({act_row['component']}):\")\n",
    "                            print(f\"     Shape: {vec.shape}\")\n",
    "                            print(f\"     Dtype: {vec.dtype}\")\n",
    "                            print(f\"     Norm: {vec.float().norm().item():.4f}\")\n",
    "                            print(f\"     Mean: {vec.float().mean().item():.4f}\")\n",
    "                            print(f\"     Std: {vec.float().std().item():.4f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ö†Ô∏è  Error loading layer {act_row['layer_index']}: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping activation loading (not available or safetensors not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Probe Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Probe Information\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('probes', False):\n",
    "    probes = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            probe_id,\n",
    "            probe_kind,\n",
    "            model_id,\n",
    "            layers_json,\n",
    "            component,\n",
    "            token_position,\n",
    "            artifact_path,\n",
    "            metrics_json\n",
    "        FROM conformity_probes\n",
    "        WHERE run_id = ?\n",
    "        ORDER BY created_at\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"üîç Trained Probes:\")\n",
    "    for idx, probe in probes.iterrows():\n",
    "        print(f\"\\n--- Probe {idx + 1} ---\")\n",
    "        print(f\"ID: {probe['probe_id'][:8]}...\")\n",
    "        print(f\"Kind: {probe['probe_kind']}\")\n",
    "        print(f\"Model: {probe['model_id']}\")\n",
    "        print(f\"Component: {probe['component']}\")\n",
    "        print(f\"Token Position: {probe['token_position']}\")\n",
    "        print(f\"Artifact: {probe['artifact_path']}\")\n",
    "        \n",
    "        if probe['metrics_json']:\n",
    "            metrics = json.loads(probe['metrics_json'])\n",
    "            print(f\"Metrics: {json.dumps(metrics, indent=2)}\")\n",
    "        \n",
    "        # Check if artifact exists\n",
    "        artifact_path = Path(RUN_DIR) / probe['artifact_path']\n",
    "        if artifact_path.exists():\n",
    "            print(f\"‚úÖ Artifact file exists\")\n",
    "            if SAFETENSORS_AVAILABLE:\n",
    "                try:\n",
    "                    weights = load_safetensors(str(artifact_path))\n",
    "                    print(f\"   Weight keys: {list(weights.keys())[:10]}...\")\n",
    "                    print(f\"   Total keys: {len(weights.keys())}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error loading: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Artifact file not found\")\n",
    "else:\n",
    "    print(\"‚ùå No probes found for this run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Probe Projections Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('probe_projections', False):\n",
    "    # Get projection statistics\n",
    "    projections = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            pr.probe_id,\n",
    "            p.probe_kind,\n",
    "            pr.layer_index,\n",
    "            COUNT(*) as projection_count,\n",
    "            AVG(pr.value_float) as mean_projection,\n",
    "            AVG(ABS(pr.value_float)) as mean_abs_projection,\n",
    "            MIN(pr.value_float) as min_projection,\n",
    "            MAX(pr.value_float) as max_projection\n",
    "        FROM conformity_probe_projections pr\n",
    "        JOIN conformity_probes p ON p.probe_id = pr.probe_id\n",
    "        WHERE p.run_id = ?\n",
    "        GROUP BY pr.probe_id, pr.layer_index\n",
    "        ORDER BY pr.probe_id, pr.layer_index\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"üìä Probe Projection Statistics:\")\n",
    "    print(projections.to_string(index=False))\n",
    "    \n",
    "    # Plot projections by layer for each probe\n",
    "    if len(projections) > 0:\n",
    "        probe_kinds = projections['probe_kind'].unique()\n",
    "        n_probes = len(probe_kinds)\n",
    "        \n",
    "        if n_probes > 0:\n",
    "            fig, axes = plt.subplots(1, min(2, n_probes), figsize=(15, 5))\n",
    "            if n_probes == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for idx, probe_kind in enumerate(probe_kinds[:2]):\n",
    "                probe_data = projections[projections['probe_kind'] == probe_kind]\n",
    "                \n",
    "                ax = axes[idx] if idx < len(axes) else axes[0]\n",
    "                ax.plot(probe_data['layer_index'], probe_data['mean_projection'], \n",
    "                       marker='o', label=probe_kind, linewidth=2)\n",
    "                ax.set_xlabel('Layer Index')\n",
    "                ax.set_ylabel('Mean Projection')\n",
    "                ax.set_title(f'{probe_kind.capitalize()} Probe Projections by Layer')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Sample projections for specific trials\n",
    "            print(\"\\nüìã Sample Projections by Trial:\")\n",
    "            sample_projections = pd.read_sql_query(\"\"\"\n",
    "                SELECT \n",
    "                    t.trial_id,\n",
    "                    t.variant,\n",
    "                    c.name as condition_name,\n",
    "                    p.probe_kind,\n",
    "                    pr.layer_index,\n",
    "                    pr.value_float\n",
    "                FROM conformity_probe_projections pr\n",
    "                JOIN conformity_trials t ON t.trial_id = pr.trial_id\n",
    "                JOIN conformity_probes p ON p.probe_id = pr.probe_id\n",
    "                JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "                WHERE p.run_id = ? AND pr.layer_index IN (10, 15, 20)\n",
    "                ORDER BY t.created_at\n",
    "                LIMIT 20\n",
    "            \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "            \n",
    "            print(sample_projections.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ùå No probe projections found for this run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Logit Lens Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Logit Lens Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('logit_lens', False):\n",
    "    # Get logit lens statistics\n",
    "    logit_stats = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            layer_index,\n",
    "            COUNT(*) as count,\n",
    "            COUNT(DISTINCT trial_id) as unique_trials\n",
    "        FROM conformity_logit_lens\n",
    "        WHERE trial_id IN (SELECT trial_id FROM conformity_trials WHERE run_id = ?)\n",
    "        GROUP BY layer_index\n",
    "        ORDER BY layer_index\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"üîç Logit Lens Statistics:\")\n",
    "    print(logit_stats.to_string(index=False))\n",
    "    \n",
    "    # Get sample top-k predictions\n",
    "    sample_logit = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            t.trial_id,\n",
    "            t.variant,\n",
    "            c.name as condition_name,\n",
    "            ll.layer_index,\n",
    "            ll.token_index,\n",
    "            ll.topk_json\n",
    "        FROM conformity_logit_lens ll\n",
    "        JOIN conformity_trials t ON t.trial_id = ll.trial_id\n",
    "        JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "        WHERE t.run_id = ? AND ll.layer_index IN (0, 10, 20, 30)\n",
    "        ORDER BY t.created_at\n",
    "        LIMIT 10\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"\\nüìã Sample Top-K Predictions:\")\n",
    "    for idx, row in sample_logit.iterrows():\n",
    "        print(f\"\\n--- Trial {row['trial_id'][:8]}... | Layer {row['layer_index']} ---\")\n",
    "        print(f\"Variant: {row['variant']} | Condition: {row['condition_name']}\")\n",
    "        \n",
    "        topk = json.loads(row['topk_json'])\n",
    "        print(\"Top predictions:\")\n",
    "        for i, pred in enumerate(topk[:5]):\n",
    "            print(f\"  {i+1}. '{pred['token']}' (prob: {pred['prob']:.4f})\")\n",
    "else:\n",
    "    print(\"‚ùå No logit lens data found for this run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Think Token Analysis (for Think Variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Think Token Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('think_tokens', False):\n",
    "    # Get think token statistics\n",
    "    think_stats = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            t.variant,\n",
    "            COUNT(DISTINCT tt.trial_id) as trials_with_think,\n",
    "            COUNT(tt.think_id) as total_think_tokens,\n",
    "            AVG(token_count.count) as avg_tokens_per_trial\n",
    "        FROM conformity_think_tokens tt\n",
    "        JOIN conformity_trials t ON t.trial_id = tt.trial_id\n",
    "        LEFT JOIN (\n",
    "            SELECT trial_id, COUNT(*) as count\n",
    "            FROM conformity_think_tokens\n",
    "            GROUP BY trial_id\n",
    "        ) token_count ON token_count.trial_id = tt.trial_id\n",
    "        WHERE t.run_id = ?\n",
    "        GROUP BY t.variant\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"üí≠ Think Token Statistics:\")\n",
    "    print(think_stats.to_string(index=False))\n",
    "    \n",
    "    # Get sample think tokens\n",
    "    sample_think = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            t.trial_id,\n",
    "            t.variant,\n",
    "            c.name as condition_name,\n",
    "            i.question,\n",
    "            o.raw_text as response,\n",
    "            GROUP_CONCAT(tt.token_text, ' ') as think_content\n",
    "        FROM conformity_think_tokens tt\n",
    "        JOIN conformity_trials t ON t.trial_id = tt.trial_id\n",
    "        JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "        JOIN conformity_items i ON i.item_id = t.item_id\n",
    "        JOIN conformity_outputs o ON o.trial_id = t.trial_id\n",
    "        WHERE t.run_id = ? AND t.variant LIKE '%think%'\n",
    "        GROUP BY t.trial_id\n",
    "        LIMIT 5\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"\\nüìã Sample Think Tokens:\")\n",
    "    for idx, row in sample_think.iterrows():\n",
    "        print(f\"\\n--- Trial {row['trial_id'][:8]}... ---\")\n",
    "        print(f\"Variant: {row['variant']} | Condition: {row['condition_name']}\")\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        think_text = row['think_content'] if row['think_content'] else \"(no think content)\"\n",
    "        print(f\"Think Content: {think_text[:300]}...\")\n",
    "        print(f\"Full Response: {row['response'][:300]}...\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No think tokens found (this is normal for non-Think variants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Intervention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Intervention Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if prerequisites.get('interventions', False):\n",
    "    # Get intervention definitions\n",
    "    interventions = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            intervention_id,\n",
    "            name,\n",
    "            alpha,\n",
    "            target_layers_json,\n",
    "            component,\n",
    "            notes\n",
    "        FROM conformity_interventions\n",
    "        WHERE run_id = ?\n",
    "        ORDER BY created_at\n",
    "    \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "    \n",
    "    print(\"üîß Intervention Definitions:\")\n",
    "    print(interventions.to_string(index=False))\n",
    "    \n",
    "    if prerequisites.get('intervention_results', False):\n",
    "        # Get intervention results\n",
    "        intervention_results = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                i.name,\n",
    "                i.alpha,\n",
    "                COUNT(*) as total_trials,\n",
    "                SUM(CASE WHEN r.flipped_to_truth = 1 THEN 1 ELSE 0 END) as flipped_count,\n",
    "                AVG(CASE WHEN r.flipped_to_truth = 1 THEN 1.0 ELSE 0.0 END) as flip_rate\n",
    "            FROM conformity_intervention_results r\n",
    "            JOIN conformity_interventions i ON i.intervention_id = r.intervention_id\n",
    "            WHERE i.run_id = ?\n",
    "            GROUP BY i.intervention_id, i.name, i.alpha\n",
    "            ORDER BY i.alpha\n",
    "        \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "        \n",
    "        print(\"\\nüìä Intervention Results:\")\n",
    "        print(intervention_results.to_string(index=False))\n",
    "        \n",
    "        # Sample intervention before/after\n",
    "        sample_intervention = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                t.trial_id,\n",
    "                c.name as condition_name,\n",
    "                i.question,\n",
    "                i.ground_truth_text,\n",
    "                o1.raw_text as response_before,\n",
    "                o1.is_correct as correct_before,\n",
    "                o2.raw_text as response_after,\n",
    "                o2.is_correct as correct_after,\n",
    "                r.flipped_to_truth,\n",
    "                interv.name as intervention_name,\n",
    "                interv.alpha\n",
    "            FROM conformity_intervention_results r\n",
    "            JOIN conformity_trials t ON t.trial_id = r.trial_id\n",
    "            JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "            JOIN conformity_items i ON i.item_id = t.item_id\n",
    "            JOIN conformity_outputs o1 ON o1.output_id = r.output_id_before\n",
    "            JOIN conformity_outputs o2 ON o2.output_id = r.output_id_after\n",
    "            JOIN conformity_interventions interv ON interv.intervention_id = r.intervention_id\n",
    "            WHERE t.run_id = ? AND r.flipped_to_truth = 1\n",
    "            LIMIT 5\n",
    "        \"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "        \n",
    "        if len(sample_intervention) > 0:\n",
    "            print(\"\\nüìã Sample Successful Interventions (Flipped to Truth):\")\n",
    "            for idx, row in sample_intervention.iterrows():\n",
    "                print(f\"\\n--- Intervention {idx + 1} ---\")\n",
    "                print(f\"Intervention: {row['intervention_name']} (alpha={row['alpha']})\")\n",
    "                print(f\"Question: {row['question']}\")\n",
    "                print(f\"Ground Truth: {row['ground_truth_text']}\")\n",
    "                print(f\"Before: {row['response_before']} (correct: {row['correct_before']})\")\n",
    "                print(f\"After: {row['response_after']} (correct: {row['correct_after']})\")\n",
    "                print(f\"‚úÖ Flipped to truth!\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No interventions found for this run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Behavioral Analysis and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Behavioral Analysis: Accuracy and Conformity Rates\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate accuracy by variant and condition\n",
    "behavioral_analysis = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        t.variant,\n",
    "        c.name as condition_name,\n",
    "        COUNT(*) as total_trials,\n",
    "        SUM(CASE WHEN o.is_correct = 1 THEN 1 ELSE 0 END) as correct_count,\n",
    "        SUM(CASE WHEN o.is_correct = 0 THEN 1 ELSE 0 END) as incorrect_count,\n",
    "        AVG(CASE WHEN o.is_correct = 1 THEN 1.0 ELSE 0.0 END) as accuracy,\n",
    "        AVG(o.latency_ms) as avg_latency_ms\n",
    "    FROM conformity_trials t\n",
    "    JOIN conformity_conditions c ON c.condition_id = t.condition_id\n",
    "    LEFT JOIN conformity_outputs o ON o.trial_id = t.trial_id\n",
    "    WHERE t.run_id = ? AND o.is_correct IS NOT NULL\n",
    "    GROUP BY t.variant, c.name\n",
    "    ORDER BY t.variant, c.name\n",
    "\"\"\", trace_db.conn, params=(RUN_ID,))\n",
    "\n",
    "print(\"üìä Behavioral Analysis:\")\n",
    "print(behavioral_analysis.to_string(index=False))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy by variant and condition\n",
    "pivot_acc = behavioral_analysis.pivot(index='variant', columns='condition_name', values='accuracy')\n",
    "pivot_acc.plot(kind='bar', ax=axes[0, 0], rot=45)\n",
    "axes[0, 0].set_title('Accuracy by Variant and Condition')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend(title='Condition')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Trial counts\n",
    "pivot_count = behavioral_analysis.pivot(index='variant', columns='condition_name', values='total_trials')\n",
    "pivot_count.plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "axes[0, 1].set_title('Trial Counts by Variant and Condition')\n",
    "axes[0, 1].set_ylabel('Number of Trials')\n",
    "axes[0, 1].legend(title='Condition')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Latency analysis\n",
    "pivot_latency = behavioral_analysis.pivot(index='variant', columns='condition_name', values='avg_latency_ms')\n",
    "pivot_latency.plot(kind='bar', ax=axes[1, 0], rot=45)\n",
    "axes[1, 0].set_title('Average Latency by Variant and Condition')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].legend(title='Condition')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Conformity rate (accuracy drop from control)\n",
    "control_data = behavioral_analysis[behavioral_analysis['condition_name'] == 'control']\n",
    "pressure_data = behavioral_analysis[behavioral_analysis['condition_name'] != 'control']\n",
    "\n",
    "if len(control_data) > 0 and len(pressure_data) > 0:\n",
    "    control_acc = control_data.set_index('variant')['accuracy']\n",
    "    pressure_acc = pressure_data.pivot(index='variant', columns='condition_name', values='accuracy')\n",
    "    \n",
    "    conformity_rate = {}\n",
    "    for variant in control_acc.index:\n",
    "        if variant in pressure_acc.index:\n",
    "            for condition in pressure_acc.columns:\n",
    "                if not pd.isna(pressure_acc.loc[variant, condition]):\n",
    "                    key = f\"{variant}_{condition}\"\n",
    "                    conformity_rate[key] = control_acc[variant] - pressure_acc.loc[variant, condition]\n",
    "    \n",
    "    if conformity_rate:\n",
    "        pd.Series(conformity_rate).plot(kind='bar', ax=axes[1, 1], rot=45)\n",
    "        axes[1, 1].set_title('Conformity Rate (Control Accuracy - Pressure Accuracy)')\n",
    "        axes[1, 1].set_ylabel('Accuracy Drop')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Generate Summary Report\n",
    "# ============================================================================\n",
    "\n",
    "summary = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_dir\": str(RUN_DIR),\n",
    "    \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "    \"prerequisites\": prerequisites,\n",
    "    \"statistics\": {\n",
    "        \"total_trials\": int(total_trials),\n",
    "        \"total_outputs\": int(total_outputs),\n",
    "        \"variants\": int(trials_summary['variant'].nunique()),\n",
    "        \"conditions\": int(trials_summary['condition_name'].nunique()),\n",
    "    },\n",
    "    \"behavioral_summary\": behavioral_analysis.to_dict('records') if 'behavioral_analysis' in globals() else [],\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = Path(RUN_DIR) / \"artifacts\" / \"analysis_summary.json\"\n",
    "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Summary report saved to: {summary_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Total Trials: {summary['statistics']['total_trials']}\")\n",
    "print(f\"Variants: {summary['statistics']['variants']}\")\n",
    "print(f\"Conditions: {summary['statistics']['conditions']}\")\n",
    "print(f\"\\nAvailable Data:\")\n",
    "for name, exists in prerequisites.items():\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {name}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cleanup\n",
    "# ============================================================================\n",
    "\n",
    "trace_db.close()\n",
    "print(\"‚úÖ Database connection closed\")\n",
    "print(\"\\nüéâ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
